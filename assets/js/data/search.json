[ { "title": "Build 3-Tier Architecture in AWS", "url": "/posts/AWS/", "categories": "Special Lecture, CLOIT", "tags": "Java, Spring", "date": "2022-09-22 14:19:00 +0900", "snippet": "" }, { "title": "REST API Project1. Project Introduce", "url": "/posts/CloneCodingProject1/", "categories": "Backend, REST API Project", "tags": "react, Backend, SW, DBMS", "date": "2022-08-25 11:31:24 +0900", "snippet": "REST API Project Chapter1. IntroduceBackground 고객들이 Coffe Bean package를 온라인 웹 사이트로 주문할 수 있도록 하는 API 개발 매일 전날 ㅇ후 2시부터 오늘 오후 2시까지의 주문을 모아서 처리한다. 커피의 종류 Columbia Brazil Serra Columbia Quindio Ethiopia Sidamo 회원을 별도로 관리하지는 않는다. email로 고객을 구분하고 주문을 받을 때 email을 같이 받아 주문을 받는다. 하나의 eamil로 하루에 여러번 주문을 해도 하나로 합쳐서 다음날 배송을 보낸다. 고객에게 “당일 오후 2시 이후의 주문은 다음날 배송을 시작합니다.” 라고 알려준다.시스템 구성도Server는 데이터베이스와 연결되게 된다.프로젝트 생성 Dependencies Web - Spring Web SQL - JDBC API SQL - MySQL Driver " }, { "title": "Tibero Lecture5. Backup", "url": "/posts/TiberoAcademyday5/", "categories": "Special Lecture, Tibero DBMS", "tags": "Tibero, Backend, SW, DBMS", "date": "2022-08-25 11:31:24 +0900", "snippet": "Backup 프로그램을 사용하다 보면 다양한 원인에 의해 작업이 비정상으로 종료되는 경우가 있는데 이를 위해서는 백업으로 데이터베이스를 보호하는 것이 중요하다. 여러가지 유형의 장애 명령문의 실패 사용자 프로세스의 실패 사용자로 인한 장애 Instance fail Media fail 백업 (하루에 한번 Export 백업 권장) 백업의 종류 논리적인 백업 : Export 툴로 하는 백업으로 데이터베이스의 논리적인 단위를 백업한다. 물리적인 백업 : COPY명령어를 통해 데이터베이스가 저장되어 있는 datafile, controlfile, archive logfile 등을 백업하는 것이다. 백업의 대상 1. Controlfile 데이터베이스를 mount할때 반드시 필요한 파일로 두개이상의 controlfile을 구성하고 서로 다른 디스크에 위치시키는 것을 권장한다. V$CONTROLFILE 다중화방법 : 데이터베이스를 mount할때 필요한 것이므로 우선 tbdown을 시킨후 controlfile을 copy 시킨 후 $TB_SIT.tip 파일에 CONTOL_FILES 파라미터를 추가한 후 tbboot로 재기동한다. OffLine Backup : O/S의 copy명령을 통해 별도의 위치에 COPY한다. OnLine Backup : 다음의 구문으로 copy한다. SQL&gt; ALTER DATABASE BACKUP CONTROLFILE TO TRACE AS [백업할 파일 경로 및 이름] REUSE NORESETLOGS; 백업의 대상 2. Redo logfile 데이터베이스의 모든 변경사항이 저장되는 파일로 최소한 2개 이상의 redo log group으로 구성되어 각 log group은 하나 이상의 redo logfile로 구성된다. V$LOG / V$LOGFILE 오늘은 다양한 방법으로 백업하고 고의로 데이터를 손상시켜 다시 복구하는 작업을 해보고자 한다. 이걸 배우면서 우영우드라마에 서버해킹당해서 고객들 개인정보 빠져나간 편이 생각났다…ㅋ :)오늘의 실습과정은 다음과 같다.0. 아카이브 로그 모드 변경1. 오프라인 백업하기 &amp; 복원하기 Tibero를 정상 종료한 후 OS의 copy명령어를 이용해 datefile, logfile, controlfile, tip file등을 백업한다. mount, open모드에서는 V$DATAFILE, V$LOGFILE 뷰를 통해 백업할 파일의 정보를 조회할 수 있다. ARCHIVELOG모드에서는 archive파일도 백업한다.2. 온라인 백업하기 &amp; 복원하기 온라인 백업과정은 다음과 같은데 처음에는 cp 명령어를 사용해 하나하나 복사를 하여 백업폴더에 넣고 이를 복원할땐 또 다시 cp명령어를 사용해 돌려놔주어야 했다. 그래서 shell script를 통해 과정을 한번에 보여주겠다.백업### TIBERO INSTANCE SHUTDOWNtbdown immediate### COPY DATABASEcp /tibero/tbdata/tibero/*.dtf /tibero/s/off_backupcp /tibero/tbdata/tibero/*.log /tibero/s/off_backupcp /tibero/tbdata/tibero/*.ctl /tibero/s/off_backupcp /tibero/tbdata/tibero/arch/*.arc /tibero/s/off_backupcp /tibero/tibero7/config/tibero.tip /tibero/s/off_backupcp /tibero/tbdata/tibero/.passwd /tibero/s/off_backup### TIBERO INSTANCE STARTtbboot ~/*.dtf : datafile 백업 ~/*.log : logfile 백업 ~/*.ctl : controlfile 백업 ~/*.arc : archive파일 백업 ~/tibero.tip ~/.passwd 파일이를 백업하는 것을 볼 수 있다. 그럼 이것을 고의로 데이터 삭제 후 복원해보자.복원### TIBERO INSTANCE SHUTDOWNtbdown abort### DELETE DATABASE rm /tibero/tbdata/tibero/*.dtf rm /tibero/tbdata/tibero/*.log rm /tibero/tbdata/tibero/*.ctl rm /tibero/tbdata/tibero/arch/*.arc rm /tibero/tibero7/config/tibero.tiprm /tibero/tbdata/tibero/.passwd ### RESTORE DATBASEcp /tibero/s/off_backup/*.dtf /tibero/tbdata/tibero cp /tibero/s/off_backup/*.log /tibero/tbdata/tibero cp /tibero/s/off_backup/*.ctl /tibero/tbdata/tibero cp /tibero/s/off_backup/*.arc /tibero/tbdata/tibero/archcp /tibero/s/off_backup/tibero.tip /tibero/tibero7/configcp /tibero/s/off_backup/.passwd /tibero/tbdata/tibero### TIBERO INSTANCE STARTtbboot이렇게 고의로 database를 지우고 다시 restore해주는 과정을 진행해주니 잘 복원이 되었다. shell script로 작성하고 하지 않고 그냥 한줄씩입력하다보면 과정을 빼먹기도 하고 tbdown abort를 잊어버리고 진행하여 잘 복원이 되지않아 고생이었다. :&gt; 기능을 잘 활용하자!온라인 백업 &amp; 복원" }, { "title": "Tibero Lecture4. Tibero Tools", "url": "/posts/TiberoAcademyday4/", "categories": "Special Lecture, Tibero DBMS", "tags": "Tibero, Backend, SW, DBMS", "date": "2022-08-24 15:57:13 +0900", "snippet": "Tibero Tools Tibero Tool이란 Tibero에서 제공하는 각종 유틸리티 도구로 이 도구들의 사용법에 대한 학습을 한다.0. Tibero Utility Tibero Utility에는 tbSQL, tbStudio, tbExport/tbImport, tbLoader, T-Up, 기타 Utility (tbrmgr, tbpc, tbdv) 이 있고 이들을 하나씩 알아보겠다.1. tbSQL tbSQL은 앞서 실습에서 계속 사용했던 기능으로 $tbsql로 실행시켜 일반적인 SQL 문장 및 tbPSM 프로그램을 입력, 편집, 저장, 실행할 수 있다.&lt;주요기능&gt; 트랜잭션 설정 및 종료 스크립트를 통한 일괄 실행 DBA에 의한 데이터베이스 관리 데이터베이스의 기동 및 종료 외부 유틸리티 및 프로그램의 실행 tbSQL 환경 설정2. tbStudio 처음 배운 Tibero tool로 마치 mySQL workbench 같은 느낌이었다. 이는 Tibero를 이용하는 개발을 돕는 GUI Tool로써 개발에 필요한 기능과 환경을 제공한다.그 중 주요기능인 Export/Import 기능을 실습해 보았다.기본적으로 editor에서는 기본 SQL문을 작성해 결과를 확인할 수 있다. 그 밖에 DB 구조와 데이터를 binary 파일로 Export 및 Import 할 수 있다.우선 초기 data를 보면 이렇게 DEPT Table과 그 속의 4개의 columns이 존재하는 것을 볼 수 있다. 그렇다면 이 자료를 Export를 통해 binary파일로 저장해놓고 Dept Table을 삭제한 후 binary파일을 import하여 다시 가져와보겠다.Export이렇게 Export File의 경로를 설정해준 후 Full Databse를 클릭해 모든 Data를 파일로 저장해 주었다. 그 후 DEPT table을 삭제하고 select문을 통해 DEPT Table이 존재하지 않는 것을 확인했다. 이 상태에서 Import를 통해 binary파일 속 DEPT Table을 가져와 보자.Import이렇게 가져올 수 있는데 파일 속 내용중 User와 Table을 입력하여 Import가 가능하고 실습에서는 Tibero user에 아까 drop한 DEPT Talbe을 Import해주었다.Import를 하고나면 다시 DEPT Table이 조회되는 것을 알 수 있다.이 과정은 putty의 shell환경에서도 tbExport/tbImport로 할 수 있다.tbExport/tbImport shell 환경에서 이루어지는 export/import로 로컬 tibero에서 full모드로 export하고 dept 테이블을 일부러 손상시킨 후 export해놓은 파일을 import해 데이터를 복원하는 과정을 진행할 것이다.다음과 같이 tibero에 tbexport, tbimport기능이 있는것을 확인할 수 있다.1. tbexport하기 우선 tbexport를 이용하여 full모드로 export를 진행하여 data의 내용을 파일로 저장한다.[tibero@T1:/tibero]$ mkdir expimp[tibero@T1:/tibero]$ cd expimptbexport를 할때 생성될 파일을 저장할 디렉토리부터 생성해준다.[tibero@T1:/tibero/expimp]$ tbexport IP=localhost PORT=8629 SID=tibero USERNAME=sys PASSWORD=tibero FULL=Y FILE=/tibero/expimp/default.dat SCRIPT=Ytbexport의 타겟이되는 서버의 IP, PORT번호, SID, USERNAME, PASSWORD를 입력해주고 본 실습에서는 FULL로 export할 것이라 FULL=Y, 그리고 이 파일이 저장될 경로와 파일이름을 지정해준다. 파일이름은 default.dat으로 tbStudio에서 진행한 Export 파일의 이름과 동일하다.그럼 /tibero/expimp 폴더에 다음과 같이 생성이 된다.2. tbImport하기 위에서 생성한 default.dat파일로 손상된 데이터를 tbImport를 통해 복원해보겠다.[tibero@T1:/tibero/expimp]$ tbimport IP=localhost PORT=8629 SID=tibero USERNAME=sys PASSWORD=tibero TABLE=TIBERO.DEPT IGNORE=Y;파일만 있으면 tbImport는 간단히 해결할 수 있다.tbLoader tbLoader는 있는 데이터를 export하고 import하는 것과 다르게 table의 구조와 table속 data의 내용파일을 저장하고 이 둘을 local에서 tbLoader를 통해 table생성을 하는 것이다. tbLoader는 Tibero 유틸리티안내서의 분리된 레코드 부분을 참고하여 실습하였다. (안내서를 잘 참고하는 것이 중요한거 같다 :&gt; )1. 테이블 생성하기 loader폴더를 만든후 tbsql을 통해 SQL에 접속해 database를 생성한다. 그럼 데이터는 없는 빈 table이 생성된다.2. control.ctl 파일 생성하기 control.ctl이란 테이블에 들어갈 데이터가 있는 파일의 경로, 이름과 이를 어떤 table에 넣을지, 등이 들어있다. 파일의 내용을 보며 살펴보자.LOAD DATAINFILE './data.dat' -&gt; ./data.dat 의 데이터를 load할거에요APPEND INTO TABLE club -&gt; club table에 넣을거에요(club table 미리 생성 필요)FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' -&gt; ','나 '\"' 로 데이터가 구분되요.LINES TERMINATED BY '|\\n' -&gt; line의 구분은 | 와 줄바꿈이에요.IGNORE 1 LINES -&gt; 첫째줄은 무시해요.( id integer external, name, masterid integer external)이런식인데 tibero test를 볼땐 무시할 첫째줄이 없었고 line의 구분이 ‘|’ 없이 줄바꿈만 되어 있어서 이렇게 작성했다. 또한 당연히 파일이름이나 table이름은 조건에 맞춰 바꿔줘야한다 :)LOAD DATAINFILE [데이터경로/이름]APPENDINTO TABLE [table 이름]FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"'LINES TERMINATED BY '\\n'3. data.dat 파일 작성하기 data.dat파일은 위에서 생성한 table에 들어갈 data들이다. 이 data들을 control.ctl파일을 통해 설정해 tbLoader를 통해 load하는 것이다.id name masterid|111115,\"DANCE MANIA\",2456|111116,\"MUHANZILZU\",2378|111117,\"INT'L\",5555 위에 설명한 control.ctl 처럼 첫째줄은 무시하고 줄구분은 ‘ ’ 와 줄바꿈으로 하는 것을 확인할 수 있다. 4. tbLoader 로 load하기[tibero@T1:/tibero/loader]$ tbloader userid=tibero/tmax@tibero control=./control.ctl마지막으로 데이터를 넣을 스키마의 아이디, 비번, alias를 입력하여 설정파일로 load해주면 끝!T-UP T-UP은 T-Studio 와 비슷한 tool이다./ T-UP 은 티맥스 테크넷에서 다운로드 받을 수 있다. 실습시, 강사가 제공한 다음 파일 실행하여 진행함.C:\\tibero\\SW1\\T-UP\\T-Up_20220810_win\\T-Up.x86_64.bat 오라클 접속을 위해 오라클 jdbc 드라이버가 필요하다.(오라클 홈페이지에서 다운로드) 강사가 공유폴더에 올려놓은 ojdbc6.jar 을 다운받아서, T-Up 의 lib 디렉토리에 넣어서 사용함 마이그레이션은 요약하면 2가지를 옮기는 작업임(DDL, DATA) SOURCE DB, TARGET DB 양쪽에 접속이 필요함(인터페이스 드라이버 파일, 접속정보) 무엇을 마이그레이션 할것인지 선택해야 함.(실습시, 오라클의 EDU 스키마를 선택함)" }, { "title": "Tibero Lecture3. Tibero Database Link", "url": "/posts/TiberoAcademyday3/", "categories": "Special Lecture, Tibero DBMS", "tags": "Tibero, Backend, SW, DBMS", "date": "2022-08-22 14:27:48 +0900", "snippet": "Tibero Database Link B 에 특화된 GateWay를 사용하여야 한다. GateWay의 역할은 B와 A의 연결을 위해 변형을 하는 역할을 한다. GateWay 구성을 위해 Client Library가 필요하고 이는 설치, 설정문제에서 보다 복잡하다.Tibero to Tibero (TtoT) 위의 사진에서 A, B가 Tibero인 경우이다. 이 경우에는 A와 B가 같아 변형이 필요없기 때문에 GateWay가 필요없다.Tibero to Oracle (TtoO) GateWay가 사용할 Oracle Interface Driver가 필요하다.Public DBLink 데이터베이스 링크를 생성한 사용자와 다른 사용자들도 데이터베이스 링크를 이용할 수 있.Public DBLink를 생성하기 위해서는 create public database link 권한이 있어야 한다. Public DBLink 생성create public database link public_tibero using [연결할 데이터베이스를 가르키는 이름];위의 괄호에 들어갈것은 tbdsn.tbr파일에 연결정보가 저장되어 있어야한다. Public DBLink 제거 drop public database link public_tibero; Private DBLink 데이터베이스 링크를 생성한 사용자만 데이터베이스 링크를 사용할 수 있다.Private DBLink를 생성하기 위해서는 create database link권한이 있어야 한다.생성과 제거는 Public DBLink와 같지만 사용하기 위해서는 권한이 필요하다는 차이가 있다.그렇다면 Tibero to Tibero 와 Tibero to Oracle을 하는 과정을 실습과 함께 알아보자.Tibero to Tibero (TtoT)«««&lt; Updated upstream:_posts/2022-08-22-TiberoAcademyday3.md다음그림은 실습진행의 architecture이다. Tibero to Tibero이니 왼쪽의 PC에 있는 Virtualbox안에 T1 서버도 Tibero환경, 오른쪽의 강사가 셋팅한 서버또한 Tibero환경이다. 따라서 두 환경사이에서는 GateWay없이 쿼리와 결과를 전달받을 수 있다.1. Tibero 클라이언트 설정 사진 속 왼쪽환경에서 오른쪽 환경에 대한 설정을 해주는 것으로 PC Virtualbox안에 T1서버의 tbdsn.tbr파일에 강사님이 셋팅한 서버의 정보를 입력해준다.다음은 실습때 진행한 강사님이 셋팅한 서버로 연결할 서버에 맞게 tbdsn.tbr파일에 추가해주면 된다.Tibero2=( (INSTANCE=(HOST=10.188.191.33) (PORT=8629) (DB_NAME=tibero) ))2. tbsql을 통해 DB Link Object 생성 서버를 설정했으니 이 서버에 접근해 DB Link Object를 생성한다.SQL&gt; create database link &lt;DB Link명&gt; connect tso &lt;접속 사용자 ID&gt;identified by &lt;접속 패스워드&gt; 2 using &lt;접속에 사용할 alias&gt;=======다음그림은 실습진행의 architecture이다. Tibero to Tibero이니 왼쪽의 PC에 있는 Virtualbox안에 T1 서버도 Tibero환경, 오른쪽의 강사님이 셋팅한 서버또한 Tibero환경이다. 따라서 두 환경사이에서는 GateWay없이 쿼리와 결과를 전달받을 수 있다. Stashed changes:_posts/2022-08-22-TiberoAcademy.md 적용한 것은 다음과 같다.SQL&gt; create database link TLINK connecto to edu identified 'edu' 2 using 'tibero'그 후 Link가 잘 되었는지 확인해보았다.이렇게 설정이 잘됐으면 연결된 서버의 data를 사용할때 @Link이름을 붙이면 된다. 그럼 data를 가져와 로컬서버에 저장해보자.3. Data가져오기 테이블과 데이터를 모두 가져오기 SQL&gt; CREATE TABLE DEPT AS FROM DEPT@TLINK; 테이블을 만들어놓고 데이터만 가져오기 SQL&gt; CREATE TABLE DEPT AS SELECT * FROM DEPT@TLINK WHERE ROWNUM &lt; 1;SQL&gt; INSERT INTO DEPT SELECT * FROM DEPT@TLINK; 이렇게 잘 들어온것을 확인할 수 있다 :)Tibero to Oracle (TtoO) 제일 처음 사진의 A가 Tibero, B가 Oracle인 환경으로 Tibero to Tibero 와 다르게 링크의 대상이 Oracle이기 때문에 Oracle을 위한 Gateway가 필요하다. Tibero는 필요한 질의를 Oracle Gateway로 전달하고 Gateway는 Oracle에 접속하여 질의를 수행한 후 Tibero에게 전송해준다.다음 그림을 보자면 우리가 Client API를 통해 Tibero에 질의를 작성하고 이 질의를 TBGW라는 Gateway를 통해 OracleClient에게 전달하고 이 질의를 수행한 후 다시 Tibero에게 주는 것이다.그렇다면 이를 수행할 수 있는 환경을 조성해보자.1. 오라클 클라이언트 파일 설치 Tibero to Tibero와 다른 것은 직접 Gateway의 설정을 해주는 것이다. 경로 설정을 통해 Tibero에서 Gateway를 통과했을때 Oracle Client로 향해 기능을 수행할 수 있도록 설정해 주는 것이다.[tibero@T1:/tibero]$ cp instantclient-basic-linux.x64-19.16.0.0.0dbru.zip /tibero[tibero@T1:/tibero]$ unzip instantclient-basic-linux.x64-19.16.0.0.0dbru.zip[tibero@T1:/tibero]$ ls -ldrwxr-xr-x 2 tibero dba 233 Aug 23 10:46 instantclient_19_16 이를 통해 zip파일의 압축을 풀어주면 instanclient_19_16 폴더가 위치하게 된다. 이것이 오라클 클라이언트 파일이다.2. Gateway 환경작업 파일을 설치했다면 이 파일에 맞춰진 gateway 환경설정을 해주어야 한다.[tibero@T1:/tibero]$ vi ~/.bash_profile 우선 /.bash_profile을 통해 gateway와 oracle의 home 경로를 설정해준다.######## TIBERO TO ORACLE DBLINK #######export TBGW_HOME=/tibero/tbgatewayexport ORACLE_HOME=/tibero/instantclient_19_16export ORACLE_SID=orclexport LIBPATH=$ORACLE_HOME:$LIBPATHexport LD_LIBRARY_PATH=$LIBPATH:$LD_LIBRARY_PATHexport PATH=$ORACLE_HOME:$PATH TBGW_HOME과 ORACLE_HOME만 본인의 환경에 맞게 설정해주면되는데 gateway의 home과 연결할 Oracle의 home 경로를 다음과 같이 설정해준다. .bash_profile을 수정할때는 source ~/.bash_profile 을 통해 반영을 꼭 해주자!3. tnsnames.ora 생성 및 설정[tibero@T1:/tibero]$ mkdir -p $ORACLE_HOME/network/admin[tibero@T1:/tibero]$ cd $ORACLE_HOME/network/admin[tibero@T1:/tibero/instantclient_11_2/network/admin]$ vi tnsnames.oraORCL = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = 10.188.191.10)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = orcl) ) ) 다음의 경로로 tnsnames.ora 파일을 생성하고 위의 txt를 추가하였다. 이는 연결된 Oracle에 대한 접속정보를 설정하는 것이다.4. Gateway 바이너리 복사[tibero@T1:/tibero]$ cp /tibero/tibero7/client/bin/gw4orcl /tibero/tbgateway gw4orcl 이 Tibero에서 제공하는 gateway기능이라고 보면 되는데 이를 /tibero/tbgateway에 위치시키고 tibero to Oracle이 가능하도록 해준다.5. Gateway 환경설정 위의 복사한 Gateway바이너리를 tbgw.cfg 파일을 통해 tibero to Oracle이 가능하도록 설정해주는 과정이다.[tibero@T1:/tibero]$ mkdir /tibero/tbgateway[tibero@T1:/tibero]$ mkdir /tibero/tbgateway/oracle[tibero@T1:/tibero]$ mkdir /tibero/tbgateway/oracle/config[tibero@T1:/tibero]$ mkdir /tibero/tbgateway/oracle/log[tibero@T1:/tibero]$ vi /tibero/tbgateway/oracle/config/tbgw.cfg 이 과정을 통해 tbgateway에 oracle 폴더와 그 속에 config, log폴더를 만들고 /tbgateway/oracle/config 폴더에 tbgw.cfg 파일을 생성해 다음을 작성한다.LISTENER_PORT=9998LOG_DIR=/tibero/tbgateway/oracle/logLOG_LVL=2MAX_LOG_SIZE=50240000 이 파일은 Gateway설정파일로 원래 Tibero에서 진행되었을때는 gateway가 필요없어서 설정해줄 필요가 없었지만 Tibero to Oracle을 할때는 사용자가 Gateway와 관련된 설정값을 변경해주어야 하기 때문에 필요한 설정이다.6. Network Alias 설정 Tibero 클라이언트의 Network Alias 설정파일인 tbdsn.tbr 파일에 Gateway의 정보를 설정해주는 과정이다.[tibero@T1:/tibero]$ vi $TB_HOME/client/config/tbdsn.tbrMOF=( (GATEWAY=(LISTENER=(HOST=localhost)(PORT=9998)) (TARGET=ORCL) (TX_MODE=GLOBAL) )) 이 과정으로 연결할 Oracle network의 정보를 입력하였다. 여길보면 MOF의 Port 번호가 tbgw.cfg에서 설정한 port번호가 같음을 알 수있다. 여기서 MOF= 이 아니라 MOF(공백)= 이라고 처음에 작성해 오류가 났었다. 이름뒤에 띄어쓰기 없이 = 을 써야한다. 7. DB Link 생성 및 확인[tibero@T1:/tibero]$ cd $TBGW_HOME티베로 유틸리티 특징 접속 정보가 필요하다. (5가지) ip port db_name username password 접속을 위한 인터페이스 드라이버 라이브러리 파일이 필요하다. tbsql : cli library 파일 tbexport, tbimport, tbloader, T-up : jdbc library 파일 tbsql을 제외한 유틸리티들은 동작을 위해 jdk가 설치되어 있어야 한다." }, { "title": "Tibero Lecture2. Tibero Install & Setting", "url": "/posts/Tibero-Academy-day2/", "categories": "Special Lecture, Tibero DBMS", "tags": "Tibero, Backend, SW, DBMS", "date": "2022-08-17 15:00:00 +0900", "snippet": "Tibero Install &amp; Setting Tibero 설치 과정을 순서대로 따라하며 실습을 진행해보았다. Tibero 설치과정은 다음과 같다. 설치 파일 준비 (Tibero 바이너리, 라이센스 파일 등) 환경설정 파일에 환경변수 설정 Tibero 바이너리 압축해제, 라이센스 파일 복사 파라미터 파일 생성용 shell 실행 Tibero 인스턴스 기동 (nomount 모드) Database 생성 Tibero 인스턴스 기동 (normal 모드) System object 생성용 shell 실행 이 순서대로 실습한 과정을 살펴보자. 실습 과정은 실행해야하는 T1 Virtual Machine과 putty가 설치되어 있는 환경에서 진행되었다.0. 환경실행 &amp; linux 명령어 T1 Virtual Machine을 실행시킨 후 putty에서 작업을 진행한다. 해당 실습은 linux를 사용하기에 linux명령어를 알아야하는데 오늘 실습때 사용하는 명령어를 몇개 소개하고 시작하겠다. 또한 linux에서 파일을 참조할때 그 파일이 위치한 폴더에서 진행하게 되면 파일 이름만으로 참조가 가능하고 그렇지 않는 경우에는 [파일위치/파일이름] 으로 참조해야 한다. 폴더 이동하기 $ cd [이동할 폴더] 파일 위치 변경하기 $ cp [옮길 파일이름] [옮길 폴더위치]$ cp [옮길 파일위치/파일이름] [옮길 폴더위치] 파일 압축풀기 $ tar -xvzf [압축풀 파일이름]$ tar -xvzf [압축풀 파일위치/파일이름] 폴더 속 파일 확인하기 $ ls -l 1. 설치 파일 준비 티베로 테크넷 https://technet.tmaxsoft.com/ko/front/main/main.do 홈페이지에 접속해 데모라이선스 신청을 클릭한다. 회원가입과 로그인 후 신청을 하면 본인 메일로 라이선스가 도착한다. 이 라이선스와 tibero7 바이너리 압축파일을 준비해 linux 환경에 저장해 놓으면 된다.2. 환경설정 파일에 환경변수 설정 vi편집기를 이용하여 아래의 text를 추가해 환경변수를 설정해 준다. $ vi ~/.bash_profile 아래 코드를 입력해 바뀐 환경변수를 적용해준다. $ source ~/.bash_profile** 환경변수가 잘 적용되었는지 확인한다. 아래의 코드에서 확인할 사항은 $TB_HOME 과 $TB_SID이고 나머지는 이 두 변수의 값에 따라 결정되는 고정값들이다. $ echo $TB_SIDtibero$ echo $TB_HOME/tibero/tibero7 ######## TIBERO ENV ########export TB_HOME=/tibero/tibero7export TB_SID=tiberoexport TB_PROF_DIR=$TB_HOME/bin/profexport PATH=.:$TB_HOME/bin:$TB_HOME/client/bin:~/tbinary/monitor:$PATHexport LD_LIBRARY_PATH=$TB_HOME/lib:$TB_HOME/client/lib:$LD_LIBRARY_PATHexport SHLIB_PATH=$LD_LIBRARY_PATH:$SHLIB_PATHexport LIBPATH=$LD_LIBRARY_PATH:$LIBPATH######## TIBERO alias ########alias tbhome='cd $TB_HOME'alias tbbin='cd $TB_HOME/bin'alias tblog='cd $TB_HOME/instance/$TB_SID/log'alias tbcfg='cd $TB_HOME/config'alias tbcfgv='vi $TB_HOME/config/$TB_SID.tip'alias tbcli='cd ${TB_HOME}/client/config'alias tbcliv='vi ${TB_HOME}/client/config/tbdsn.tbr'alias tbcliv='vi ${TB_HOME}/client/config/tbnet_alias.tbr' alias tbdata='cd $TB_HOME/tbdata'alias tbi='cd ~/tbinary'alias clean='tbdown clean'alias dba='tbsql sys/tibero'alias tm='cd ~/tbinary/monitor;monitor;cd -'위의 코드에서 변동되는 요소는 두가지이다. TB_SID : Tibero System Identify 서비스 이름 TB_HOME : Tibero 소프트웨어가 설치될 디렉토리이 두 요소를 필요에 맞게 변경해 환경을 설정하면 된다.3. Tibero 바이너리 압축해제, 라이센스 파일 복사처음에 준비해뒀던 설치파일들이다. 이 파일들이 저장되있는 폴더로 간 후 이를 알맞은 폴더로 이동시켜 줄 것이다. 우선 티베로바이너리파일을 /tibero 로 옮겨준다. $ cp [티베로 바이너리파일 이름] /tibero$ cd /tibero 이 파일의 압축을 풀어준다. 그럼 Tibero가 설치되는 것이다. 압축을 해제하고 나면 /tibero 폴더에 Tibero7 폴더가 만들어지는데 이것이 바로 처음 환경설정때 $TB_HOME을 /tibero/tibero7이 되는 것이다. $TB_HOME과 Tibero가 설치된 위치를 맞춰주어야 한다. $ tar -xvzf [티베로 바이너리파일 이름] 압축을 풀고 나면 $TB_HOME에 license라는 폴더가 있을 것이다. 이 폴더에 처음 티맥스 홈페이지에서 전송받은 라이선스를 위치시킨다. $ cp [라이선스이름] $TB_HOME/license 4. 파라미터 파일 생성용 shell 실행$ cd $TB_HOME/config$ ./gen_tip.sh그 후 초기 환경파일인 파라미터 파일 생성을 해주는 shell을 실행한다. 이 shell을 실행시키면 $TB_SID.tip 파일과 tbdsn.tbr 파일이 생성되고 두 파일은 다음과 같다. $TB_SID.tip : Tibero 파라미터 파일 tbdsn.tbr : Tibero Client 접속 설정 파일이 두 파일또한 환경변수 설정처럼 사용자가 설정하여 사용할 수 있고 실습환경에 맞춰 변경해준다. vi $TB_HOME/config/$TB_SID.tip 을 입력해 수정 DB_NAME=tiberoLISTENER_PORT=8629CONTROL_FILES=\"/tibero/tbdata/tibero/c1.ctl\",\"/tibero/tbdata/tibero/c2.ctl\"DB_CREATE_FILE_DEST=/tibero/tbdata/tiberoLOG_ARCHIVE_DEST=/tibero/tbdata/tibero/archMAX_SESSION_COUNT=20TOTAL_SHM_SIZE=600MMEMORY_TARGET=1G 코드의 내용은 다음과 같다. DB_NAME : 데이터베이스 이름 LISTENER_PORT : 리스너가 사용할 포트번호 CONTROL_FILES : 컨트롤 파일이 존재하는 위치 (절대경로) MAX_SESSION_COUNT : 데이터베이스에 접속 가능한 최대 세션 수 TOTAL_SHM_SIZE : 데이터베이스의 인스턴스 내에서 사용할 전체 공유 메모리의 크기 MEMORY_TARGET : 데이터베이스가 사용할 수 있는 메모리의 총 크기 cat $TB_HOME/client/config/tbdsn.tbr : tbdsn.tbr 파일은 따로 수정할 것이 없어서 cat 명령어로 확인만 해주었다. 하지만 DB_NAME이나 새로운 $TB_SID를 설정해 접속하고자 한다면 수정이 필요하다.5. Tibero 인스턴스 기동 (nomount 모드)현재 database가 생성되기 전이기 때문에 Tibero를 인스턴스까지 기동을 하여 database를 추가해 주어야 한다. 인스턴스 기동을 하게 되면 nomount 모드로 시작이 되고 sys user로만 접속이 가능하게 된다. Tebero를 nomount로 부팅```shell$ tbboot nomountlistener port = 8629change core dump dir to /tibero/tibero7/bin/profTibero 7Copyright (c) 2008, 2009, 2011 Tibero Corporation. All rights reserved.Tibero instance started suspended at NOMOUNT mode.2. System 유저로 접속 (nomout모드는 System유저로만 접속이 가능하다.)```shell$ tbsql sys/tiberotbSQL 7TmaxTibero Corporation Copyright (c) 2020-. All rights reserved.Connected to Tibero.SQL&gt;6. Database 생성이렇게 System 유저로 접속 후 SQL 스크립트에서 Database를 생성한다.SQL&gt; CREATE DATABASE USER sys IDENTIFIED BY tibero MAXDATAFILES 256 CHARACTER SET MSWIN949 -- UTF8, EUCKR, ASCII ... NATIONAL CHARACTER SET UTF16 LOGFILE GROUP 0 'log01.log' SIZE 50M, GROUP 1 'log11.log' SIZE 50M, GROUP 2 'log21.log' SIZE 50M MAXLOGFILES 100 MAXLOGMEMBERS 2 NOARCHIVELOG DATAFILE 'system001.dtf' SIZE 100M AUTOEXTEND ON NEXT 64M MAXSIZE 3G DEFAULT TEMPORARY TABLESPACE TEMP TEMPFILE 'temp001.dtf' SIZE 100M AUTOEXTEND ON NEXT 64M MAXSIZE 3G EXTENT MANAGEMENT LOCAL AUTOALLOCATE UNDO TABLESPACE UNDO DATAFILE 'undo001.dtf' SIZE 200M AUTOEXTEND ON NEXT 64M MAXSIZE 3G EXTENT MANAGEMENT LOCAL UNIFORM SIZE 128k DEFAULT TABLESPACE USR DATAFILE 'usr001.dtf' SIZE 50m AUTOEXTEND ON NEXT 64m MAXSIZE 3G SYSSUB DATAFILE 'syssub001.dtf' SIZE 50m AUTOEXTEND ON NEXT 64M MAXSIZE 3G ;Database created.SQL&gt; Q7. Tibero 인스턴스 기동 (normal 모드)데이터베이스를 추가한 후 다시 tibero를 기동한다.$ tbdownTibero instance terminated (NORMAL mode).$ tbbootlistener port = 8629change core dump dir to /home/tibero/tibero7/bin/profTibero7Copyright (c) 2008, 2009, 2011 Tibero Corporation. All rights reserved.Tibero instance started up (NORMAL mode).8. System object 생성용 shell 실행그 후 이 과정이 중요한데 현재 Tibero 에는 데이터베이스의 파일만이 저장이 되어있기 때문이 이 Data의 Dictionary, system 패키지를 생성해야 우리가 데이터를 조회하고 기능을 수행할 수 있다.$ cd $TB_HOME/scripts$ sh system.shCreateing ............sys의 패스워드는 tibero, syscat의 비밀번호는 syscat으로 입력후 다른 항목들은 모두 Y를 입력하여 생성해주었다.이 과정이 끝나면 환경설정은 모두 마쳤고 tbsql sys/tibero 를 통해 접속하여 sql문을 실행시킬 수 있다. 과정들이 하나라도 빠지거나 설정을 자칫 잘못했을 때 오류가 생기고 접속이 안되어서 꽤나 어려운 환경설정과정이었다." }, { "title": "Tibero Lecture1. Tibero Architecture", "url": "/posts/Tibero-Academy-day1/", "categories": "Special Lecture, Tibero DBMS", "tags": "Tibero, Backend, SW, DBMS", "date": "2022-08-16 15:00:00 +0900", "snippet": "Tibero ArchitectureTibero Instance Tibero Database 서버는 Tibero Instance와 Database로 구성된다. Database는 데이터 파일들의 집합이고 Instance는 이 데이터베이스를 관리하는 것이다.Tibero Process 대규모 사용자 접속을 수용하는 다중 프로세스 및 다중 스레드 기반의 아키텍쳐 구조이다.LSNR (Listener), WPROC (Worker Process), Background process 로 구성된다.Tibero process 동작 과정 Client가 접속을 요청한다. Listener가 현재 빈 WTHR이 있는 프로세스를 찾아 접속요청을 CTHR에게 넘겨준다. 요청받은 CTHR은 자기 자신의 WTHR 상태를 체크해 일하지 않는 WTHR에게 할당한다. WTHR은 Client와 인증 절차를 걸쳐 세션을 시작한다. 연결된 후의 Client의 동작여부는 중요하지 않다. 인증절차에는 IP포트번호, 데이터베이스이름, user명, password등의 정보가 필요하다.Listener Client의 접속요청을 가장 처음에 받아 유효한 Worker Process에 할당하는 중계역할을 하고 이는 별도의 실행 파일인 tblistener를 사용한다.Worker Process ( Foreground Process ) Client와 실제 통신을 하여 Client의 요구사항을 처리하는 프로세스이다. CTHR (Control Thread) 각 Working Process마다 하나씩 생성되고 서버가 시작될때 지정된 개수의 Worker Thread를 생성한다. 시그널처리를 담당한다. I/O Multiplexing을 지원하고 Worker Thread대신 메시지 송/수신 역할을 수행하기도 한다. WTHR (Worker Thread) 각 Worker Process마다 여러개가 생성된다. Client가 보내는 메시지를 받아 처리하고 그 결과를 return한다. SQL Parsin, 최적화, 수행 등의 DBMS가 해야 하는 대부분의 일을 직접적으로 처리하는 곳이다. Background Process 사용자의 요청을 직접 받아들이는 것이 아니라 Worker Thread나 다른 배경 프로세스가 요청할 때, 혹은 정해진 주기에 따라 동작하며 주로 시간이 오래걸리는 디스크의 작업을 담당한다. 독립된 프로세스로서 사용자의 요청과 비동기적으로 동작한다. Backgrount Process의 종류 감시 프로세스 (MONP : monitor process) Tibero 가동 시 제일 먼저 생성되고 제일 마지막에 종료되어 프로세스들을 생성하고 주기적으로 그 프로세스들의 상태를 점검해주는 프로세스이다. 교착상태 도 검사해주는 특징이 있다. 매니저 프로세스 (MGWP : manager worker process) 시스템을 관리해주는 역할을 하고 접속요청을 받았을때 예약된 워크 스레드에 접속을 할당해준다. 하지만 이는 sys계정만 접속을 허용하여 리스너를 거치지 않고 스페셜 포트를 통해 직접 접속을 도와준다. 에이전트 프로세스 (AGNT : agent process) 시스템 유지를 위해 주기적으로 처리해야하는 Tibero 내부의 작업을 담당한다. 내부작업의 판단을 맡고 있고 판단 후 워커프로세스에게 의뢰하는 구조이다. 다중 스레드 기반으로 동작하며 서로다른 용도의 업무를 스레드별로 나누어 수행한다. 데이터베이스 쓰기 프로세스 (DBWR : database writer) 데이터베이스에서 변경된 내용을 디스크에 기록하는 일과 연관된 스레드들이 모여있다. 주기적으로 사용자가 변경한 블록을 디스크에 기록한다. 리두 로그를 디스크에 기록하고 두 스레드를 통해 데이터베이스의 체크포인트과정을 관할하는 체크포인트 스레드이다. 복구 프로세스 (RCWP : recover worker process) 복구 전용 프로세스로 Crash / Instance Recovery 를 수행한다. Tibero Shared Memory (TSM) 인스턴스에 대한 데이터와 제어정보를 가지는 공유메모리 영역이다. 사용자와 동시에 데이터를 공유한다. Database buffer, Redo Log Buffer, SQL Cache, Data Dictionary Cache로 구성된다. Background Process는 인스턴스가 시작될 때 TSM영역을 할당하고 인스턴스가 종료되면 할당이 해제된다. TSM의 전체 크기는 인스턴스가 시작될 때 생성되어 고정된다." }, { "title": "Day18.JDBC & Spring", "url": "/posts/SpringBoot-JDBC-day18/", "categories": "Backend, SpringBoot", "tags": "SpringBoot, Backend, SW, JDBC", "date": "2022-08-11 12:00:00 +0900", "snippet": "[Day18] JDBC &amp; SpringData Connection Pool (DBCP) Data와 관련된 connection을 pool에 미리 만들어 두고 필요할 때마다 가져와 사용하는 방식이다. pool에서 connection을 가져온다. connection을 사용한다. connection을 pool에 반환한다.HikariCP HikariCP는 2012년도경에 Brett Wooldridge가 개발한 매우 가볍고 빠른 JDBC Connection Pool이다.HikariCP를 사용할때는 기존의 연결방식과 조금 다르게 DB와 연결한다.// 기존의 연결 방식var connection = DriverManager.getConnection(\"jdbc:mysql://localhost/order_mgmt\", \"user_id\", \"user_password\");// HikariCP를 사용할때 연결 방식var connection = dataSource.getConnection(); @TestMethodOrder(MethodOrderer.OrderAnnotation.class) 이 코드를 Test Class에 입력하면 Order(n)을 통해 Test할 순서를 정해줄 수 있다. JDBC Template 꼭 변경할 코드만 변경하여 간결하게 코드를 작성하게 해주는 것이 JDBC Templete이다.@Overridepublic List&lt;Customer&gt; findAll() { List&lt;Customer&gt; allCustomers = new ArrayList&lt;&gt;(); try ( var connection = dataSource.getConnection(); var statement = connection.prepareStatement(\"select * from customers\"); var resultSet = statement.executeQuery(); ) { while (resultSet.next()) { mapToCustomer(allCustomers, resultSet); } } catch (SQLException throwable) { logger.error(\"Got error while closing connection\", throwable); throw new RuntimeException(throwable); } return allCustomers;}이렇게 작성된 코드를 Jdbc Template을 사용하면@Overridepublic List&lt;Customer&gt; findAll() { return jdbcTemplate.query(\"select * from customer\", customerRowMapper);}이렇게 훨씬 간단하게 findAll을 작성할 수 있다. 하지만 이를 위해서는 customerRowMapper를 정의해 주어야 한다.private static final RowMapper&lt;Customer&gt; customerRowMapper = (resultSet, i) -&gt; { var customerName = resultSet.getString(\"name\"); var email = resultSet.getString(\"email\"); var customerId = toUUID(resultSet.getBytes(\"customer_id\")); var lastLoginAt = (resultSet.getTimestamp(\"Last_login_at\") != null) ? resultSet.getTimestamp(\"last_login_at\").toLocalDateTime() : null; var createdAt = resultSet.getTimestamp(\"created_at\").toLocalDateTime(); return new Customer(customerId, customerName, email, lastLoginAt, createdAt); };이렇게 RowMapper를 추가해주면 findAll, findById등의 코드를 query문만 바꿔주며 더 쉽게 작성할 수 있다.findById 코드도 한번 바꿔보자.@Overridepublic Optional&lt;Customer&gt; findById(UUID customerId) { try{ return Optional.ofNullable(jdbcTemplate.queryForObject(\"select * from customers WHERE customer_id = UUID_TO_BIN(?)\", customerRowMapper, customerId.toString().getBytes())); } catch(EmptyResultDataAccessException e){ logger.error(\"Got empty result\", e); return Optional.empty(); }}이렇게 jdbcTemplate.queryForObject를 사용하게 되는데 이는 findById 뿐 아니라 한 건의 데이터를 사용해 구하는 findByEmail, findByName등의 기능을 모두 처리할 수 있다.그렇다면 update, insert 처럼 조회가 아닌 갱신을 필요로 하는 기능은 어떻게 할까? update를 예시로 보겠다.@Overridepublic Customer update(Customer customer) { var update = jdbcTemplate.update(\"UPDATE customers SET name = ?, email = ?, last_login_at = &gt; WHERE customer_id = UUID_TO_BIN(?)\", customer.getName(), customer.getEmail(), customer.getLastLoginAt() != null ? Timestamp.valueOf(customer.getLastLoginAt()) : null, customer.getCustomerId().toString().getBytes()); if (update != 1) { throw new RuntimeException(\"Nothing was updated\"); } return customer;}jdbcTemplate.update를 사용하게 되는데 query문을 작성하고 ? 에 들어갈 것을 차례로 나열해 주면 된다. 그 후 update가 없을때의 예외처리까지 해주면 훨씬 간단한 코드가 된다.이와 같이 jdbcTemplate를 이용해 코드를 수정해 보았는데 jdbcTemplate의 용도와 같이 꼭 입력해야하는 query, 변수등의 정보만 입력하여 기능을 구현할 수 있어 훨씬 간단하게 작성할 수 있다." }, { "title": "Day17.JDBC & JAVA", "url": "/posts/SpringBoot-JDBC-day17/", "categories": "Backend, SpringBoot", "tags": "SpringBoot, Backend, SW, JDBC", "date": "2022-08-08 17:00:00 +0900", "snippet": "[Day17] JDBC &amp; JAVA JDBC는 Java Data Base Connecter의 약자로 Java Application과 DataBase를 연결할때의 표준을 알려준다. 백엔드 개발자는 JDBC API를 이용하여 Query에 요청을 하는 등의 작업을 수행한다. JDBC Type 4가지 중 MySQL에서 제공하는 JDBC Type 4를 공부할 예정이다. JDBC Operation FLOW 우선 DB로 부터 Connection을 받아온다. Connection으로 부터 Statement를 만든다. Statement를 이용해 실행시킨다. 문제가 있다면 문제를 해결하고 Statement를 통해 쿼리를 실행하여 ResultSet을 받아오거나 update를 실행한다. Statement, Connection, ResultSet을 종료해준다. 마지막 과정인 Statement, Connection, ResultSet을 종료하는 부분이 가장 중요하다. JDBC 준비하기우선 JDBC를 준비하는 과정에서 굉장히 많은 삽질을 했다. 결론은 처음 MySQL Installer자체를 잘못 설치했기 때문이었다. 나는 window 환경에서 설치를 하였고 이 블로그를 참고하여 깔끔하게 해결하였다.https://blog.naver.com/tipsware/221303627201JDBC CRUD 실습다음과 같이 MySql을 이용해 database를 생성해주었다. 그 후 이 데이터베이스를 가져와 console에 출력하는 실습을 진행해보자.public static void main(String[] args) throws SQLException{ Connection connection = null; Statement statement = null; ResultSet resultSet = null; try { connection = DriverManager.getConnection(\"jdbc:mysql://localhost/order_mgmt\", \"DB_ID\",\"DB_PASSWORD\"); statement = connection.createStatement(); resultSet = statement.executeQuery(\"select * from customers\"); while(resultSet.next()){ var name = resultSet.getString(\"name\"); var customer_Id = UUID.nameUUIDFromBytes(resultSet.getBytes(\"customer_id\")); logger.info(\"customer id -&gt; {}, name -&gt; {}\",customer_Id,name); } } catch (SQLException throwables) { logger.error(\"Got error while closing connection\", throwables); } finally { try{ // 닫아주는 부분이 중요하다. if(connection != null) connection.close(); if(statement != null) statement.close(); if(resultSet != null) resultSet.close(); } catch(SQLException exception){ logger.error(\"Got error while closing connection\", exception); } }다음 코드에서는 Connection, Statement, ResultSet 을 try문 밖으로 빼, 우선 선언해주었다. 이는 try문이 끝나고 이 세가지 요소를 종료해주는 역할을 따로 수행해 주어야 하기 때문이다.그렇다면 언제나 Connection, Statement, ResultSet을 열고 닫아줘야 하는데 이를 도와주는 것은 없을까?try ( var connection = DriverManager.getConnection(\"jdbc:mysql://localhost/order_mgmt\", \"DB_ID\",\"DB_PASSWORD\"); var statement = connection.createStatement(); var resultSet = statement.executeQuery(\"select * from customers\");) { while (resultSet.next()) { var name = resultSet.getString(\"name\"); var customer_Id = UUID.nameUUIDFromBytes(resultSet.getBytes(\"customer_id\")); logger.info(\"customer id -&gt; {}, name -&gt; {}\", customer_Id, name); }} catch (SQLException throwables) { logger.error(\"Got error while closing connection\", throwables);}try 부분을 이렇게 수정해줄 수 있다. Java Application에서는 AutoCloser를 제공해 자동으로 종료를 해주기 때문에 이렇게 작성해주어도 자동으로 열고 닫음이 가능한 것이다.위의 기능은 customer table의 전체를 보여주는 기능인데 customer_id의 list를 보여주는 기능을 구현해 보겠다.private final String SELECT_ALL_SQL = \"select * from customers\";public List&lt;UUID&gt; findAllIds(){ List&lt;UUID&gt; uuids = new ArrayList&lt;&gt;(); try ( var connection = DriverManager.getConnection(\"jdbc:mysql://localhost/order_mgmt\", \"DB_ID\",\"DB_PASSWORD\"); var statement = connection.prepareStatement(SELECT_ALL_SQL); var resultSet = statement.executeQuery(); ) { while (resultSet.next()) { var customerName = resultSet.getString(\"name\"); var customer_Id = UUID.nameUUIDFromBytes(resultSet.getBytes(\"customer_id\")); var createdAt = resultSet.getTimestamp(\"created_at\").toLocalDateTime(); uuids.add(customer_Id); } } catch (SQLException throwables) { logger.error(\"Got error while closing connection\", throwables); } return uuids;}public static void main(String[] args) throws SQLException{ var customerRepository = new JdbcCustomerRepository(); var count = customerRepository.deleteAllCustomers(); logger.info(\"deleted count -&gt; {}\",count); var customerId = UUID.randomUUID(); logger.info(\"created customerId -&gt; {}\", customerId); customerRepository.insertCustomer(customerId, \"new-user\", \"new-user@gmail.com\"); customerRepository.findAllIds().forEach(v -&gt; logger.info(\"Found customerId : {}\",v));} 이 코드를 간단히 보면 우선 SELECT_ALL_SQL로 sql query를 작성해준다. customers table의 전체를 보겠다는 뜻이다. 저 query문이 바로 statement인데 root의 id, passwoord로 jdbc에 connection한 후 그 connection과 query문으로 statement를 불러온다. statment의 결과를 resultSet에 저장하게 되는데 이 resultSet이 끝날때까지 돌면서 결과를 저장한다. 이 결과 모든 cusotmer_id 의 list가 uuids에 저장되 return 된다.하지만 여기서 문제점이 생긴다. 아래의 main문을 보면 우선 중복 insert를 방지하기 위해 customerRepository.deleteAllCustomers()를 해주었다.그 후 랜덤한 UUID를 설정해 생성된 customerId와 insert 후 table에 있넌 customerId는 같아야하는데 결과는 같지 않다. 왜일까?정답은 JDBC Type의 차이때문인데 findAllIds에서 UUID를 받아올때 Byted를 UUID로 변경하여 가져오게 된다. 하지만 이 과정에서 main에서 생성한 UUID는 version 4이고 findAllIds의 UUID.nameUUIDFromBytes는 version 3이기 때문에 차이가 발생한다.그래서 아래와 같이 버전을 맞춰 준후 findAllIds의 UUID를 받아오는 과정을 수정해주어야한다. version 4로 하여금 byte가 UUID로 바뀌게하여 이를 uuids list에 추가해주도록 수정해주었다.static UUID toUUID(byte[] bytes){ var byteBuffer = ByteBuffer.wrap(bytes); return new UUID(byteBuffer.getLong(),byteBuffer.getLong());}다음 코드를 수정한 후 UUID.nameUUIDFromBytes(resultSet.getBytes(“customer_id”))가 아닌 toUUID(resultSet.getBytes(“customer_id”) 를 통해 customerId를 저장하면 된다.이와 같이 오늘은 delete, select, insert 등과 mySql query로 작성할 수 있는 것을 Java Application에서 작성하여 작업을 수행해 보았다." }, { "title": "Day16.SpringTest", "url": "/posts/SpringBoot-day16/", "categories": "Backend, SpringBoot", "tags": "SpringBoot, Backend, SW, SpringTest", "date": "2022-08-03 15:00:00 +0900", "snippet": "[Day16] SpringTest1. 소프트웨어 테스팅 주요 이해관계자들에게 시험대상의 품질에 관한 정보를 제공하는 조사과정이다. 소프트웨어의 결함이 있는지 찾는 과정이라고 생각할 수 있다.이 테스팅에는 여러 종류의 testing level이 존재하는데 일반적으로 backend Engineer의 경우 단위테스트 (unit test)를 거의 무조건 작성하게 되고 통합테스트도 일부 작성한고 한다.Unit Test : 단위테스트 보통 모든 class에 대해 한 class당 한 test를 작성하여 단위테스트를 진행한다. SUT(System under Test)테스트하는 대상으로 단위테스트의 단위 부분이다. 보통 class들이 SUT가 되고 이 안에는 method, property, constructor들이 있고 주로 method를 테스트하게 된다. class속 기능들을 테스트한다고 보면 된다.Unit Test는 이러한 구조를 띄는데 예를 들어서 계산기 프로그램이 있다고 가정했을때 SUT는 이 계산기 프로그램이 되고 Method 는 덧셈, 뺄셈, 곱셈, 나눗셈 등의 기능이 된다. 숫자, 계산식을 GIVEN으로 주고 전제조건, WHEN에 따라 Method들이 호출되고 THEN으로 출력값이 나와 이를 테스트할 수 있는 것이다.이렇게되면 새로운 기능이 추가가 되었을때도 앞선 기능들에 대한 확신이 되는 것이다.또한 코드들을 일일이 살펴보지않고 테스트명세만 봐도 이 프로그램에 대한 신뢰가 생긴다.Itegration Test : 통합테스트 통합테스트는 내 코드가 다른 의존관계와 연동이 잘되는지 테스트하는 것이다.JUnit JUnit의 기능 매 단위 테스트시마다 테스트 클래스의 인스턴스가 생성되어 독립적인 테스트가 가능하다. 애노테이션을 제공해서 테스트 라이프 사이클을 관리하게 해주고 테스트 코드를 간결하게 작성하도록 지원한다. 테스트 러너를 제공하여 Intellij / eclipse / maven 등에서 테스트 코드를 쉽게 실행하게 해준다. assert로 테스트 케이스의 수행 결과를 판별하게 해준다. -&gt; assertEquals(예상값, 실제값) 결과는 성공(녹색), 실패(붉은색) 으로 표시해준다. project.test.java에서 test를 작성할 수 있는데 test하고 싶은 class에서 ctrl+Insert로 test class를 바로 생성할 수 있다. 그렇다면 test를 작성하는 방식을 알아보자.import static org.junit.jupiter.api.Assertions.* 나 import static org.hamcrest.Matchers.* 을 사용할 수 있는데 개인적으로 후자가 더 좋은 것 같아 간단히 코드를 소개하고자 한다.@Test@DisplayName(\"여러 hamcrest matcher 테스트\")void hamcrestTest(){ assertEquals(2, 1+1); assertThat(1+1, equalTo(2)); //actual이 2와 같다. assertThat(1+1, is(2)); //actual이 2다. assertThat(1+1, anyOf(is(1), is(2))); //acture이 1이거나 2다. assertNotEquals(1,1+1); assertThat(1+1,not(equalTo(1))); //actual은 1과 다르다.}이런식으로 작성할 수 있는데 assertEquals / assertNotEquals가 jupiter api이고 그 밑에 assertThat이 동일한 의미의 hamcrest 이다. jupiter api보다 훨씬 코드의 의미를 직관적으로 파악할 수 있는 것 같다.또한 hamcrest는 list와 같은 요소를 test할 때 효과적이다.void hamcrestListMatcherTest(){ var prices = List.of(1,2,3); assertThat(prices,hasSize(3)); // List size가 3이다. assertThat(prices, everyItem(greaterThan(0))); //List요소가 모두 0보다 크다. assertThat(prices, containsInAnyOrder(2,3,1)); //순서상관없이 2,3,1이 있다. assertThat(prices,contains(1,2,3)); // 1,2,3 순서로 list가 있다. assertThat(prices,hasItem(2)); //List요소 중 2가 있다.}보는 것과 같이 list의 size나 요소들을 test하기에 굉장히 편리하다.테스트 더블 의존 구성요소를 사용할 수 없을때 테스트 대상 코드와 상호작용하는 객체이다.SUT속 객체가 직접테스트가 안될때 가짜 객체를 만들어 테스트를 하는 것이다. 테스트 더블은 Mock(mock, spy)와 Stub(stub, dummy, fake)로 나뉜다. 쉽게 말해 Mock은 행위검증, Stub은 상태검증이라고 한다.Mock Object : 모의 객체 단위테스트의 테스트더블을 설명했었는데 이 테스트더블의 대상코드인 SUT와 상호작용하기 위한 것이 바로 이 Mock Object이다. Stub이 가짜 객체라면 Mock은 호출에 대한 반응을 하는지 알아보는 객체이다." }, { "title": "Day15.Spring Framework (5)", "url": "/posts/SpringBootStart-day15/", "categories": "Backend, SpringBoot", "tags": "SpringBoot, Backend, SW", "date": "2022-08-01 16:00:00 +0900", "snippet": "[Day15] Spring Framework 핵심개념 (5)Logging 시스템을 작동할 때 시스템의 작동 상태의 기록과 보존, 이용자의 습성 조사 및 시스템 동작의 분석 등을 하기 위해 작동중의 각종 정보를 기록해야하는데 이 기록을 만드는 것이 로깅이다.로그 시스템의 사용에 관계된 인련의 [사건]을 시간의 겨과에 따라 기록하는 것이다. 버그가 발생했을때 되돌려 보는 용도로 사용하는 것이 대표적이다. 실습때 사용하던 println/print를 사용하지 않고 이 log를 사용한다.Java Loggin Framework java.util.logging Apache Commons logging Log4J Logback SLF4J (Simple Logging Facade for Java)이 Loggin Framework중에 Log4J, Logback을 주로 사용했었는데 두개를 병행해서 쓰고 싶어 나온 것이 SLF4J 이다.SLF4J Logging Framework들을 추상화 해놓은 것이다. Facade Pattern을 이용한 Logging Framework이다. Facade 패턴 : 많은 서브시스템을 거대한 클래스로 만들어 감싸서 편리한 인터페이스를 제공하는 것이다.SLF4J는 새로운 Loggin Framework이 나올때마다 Log 코드를 수정해야 하는 불편함을 없애준다. SLF4J는 Binding모듈과 설정만 바꿔주면 된다는 장점이 있다.로깅 프레임워크의 Binding 모듈 SLF4J가 다양한 로깅 프레임워크를 지원하는데 이는 바인딩 모듈을 통해서 처리된다. 바인딩 모듈은 로깅 프레임워크를 연결하는 역할을 한다. Log Level trace debug info warn error 보통 trace나 debug는 배포과정에서가 아닌 개발과정에서 주로 사용한다.주로 info를 base로 자주 사용한다.그렇다면 이 log를 어떻게 사용하는지 알아보자.private static final Logger log = LoggerFactory.getLogger(\"org.prgrms.kdt.OrderTester\");private static final Logger log = LoggerFactory.getLogger(OrderTester.class);static으로 사용하여 logger는 단 하나 라는 것을 의미해주고 class의 이름 전체를 가져와야 한다.//org.prgrms.kdt // SET WARN//org.prgrms.kdt.A =&gt; WARN//org.prgrms.kdt.voucher =&gt; WARN =&gt; SET INFO =&gt; INFO”.”을 기준으로 set이 되기때문에 ~.kdt에서 WARN을 설정하면 하위 클래스들은 자동으로 WARN으로 설정이 된다. 따라서 ~.kdt.voucher는 INFO로 설정하고 싶다고 하면 ~.voucher를 Info로 다시 set해줘야 한다.logger.info(\"logger name =&gt; {}\",logger.getName());logger.info(MessageFormat.format(\"logger name =&gt; {0}\", logger.getName()));또한 log를 사용할때는 편리한 점이 MessageFormat을 사용하지 않아도 “{}”를 순서대로 인식해 값을 넣을 수 있다.이 처럼 springboot는 설정없이 log를 사용할 수 있다. 그럼 직접 logback설정파일을 넣어 이 설정에 의해 log가 출력되도록 해보자.Loggerlogback 설정하기 logback-test.xml 파일을 먼저 찾습니다. 없다면 logback.groovy 을 찾습니다. 그래도 없다면 logback.xml을 찾습니다. 모두 없다면 기본 설정 전략을 따릅니다. BasicConfiguration우선 본 실습파일에는 1,2,3 이 해당이 안되기에 main.resources에 logback.xml을 추가한 후 https://logback.qos.ch/manual/configuration.html 여기서 Basic Configuration file 을 가져온다. Basic Configuration file&lt;configuration&gt; &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;!-- encoders are assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --&gt; &lt;encoder&gt; &lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level=\"debug\"&gt; &lt;appender-ref ref=\"STDOUT\" /&gt; &lt;/root&gt;&lt;/configuration&gt;이를 입맛대로 변경해주면 되는데&lt;!-- logback.xml --&gt;&lt;configuration&gt; &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;!-- encoders are assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --&gt; &lt;encoder&gt; &lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;logger name = \"org.prgrms.kdt\" level = \"debug\" /&gt; &lt;root level=\"warn\"&gt; &lt;appender-ref ref=\"STDOUT\" /&gt; &lt;/root&gt;&lt;/configuration&gt;이때 root level = “…“ 는 전체 class에 대한 log의 level을 설정해주게 된다. root level=”warn” 이라 설정했으니 warn이상의 log는 모두 나오는 것이다.하지만 org.prgrms.kdt 하위 class는 debug이상으로 나오게 하고싶다면?logger name = “org.prgrms.kdt” level = “debug” 를 추가해 설정해 주어야한다. 이것이 바로 위에서 말했던 “.”기준으로 log level이 설정된다고 했던 부분을 적용한 것이다.logback.xml의 messageformat을 작성하는 부분도 살펴보자.&lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt; {HH:mm:ss.SSS} : log가 남겨진 시간 [%thread] : DEBUG, INFO 와 같은 thread 이름 %-5level : -n 이면 왼쪽정렬에 n글자 / n 이면 오른쪽정렬에 n글자 %logger{36} : org.prgrms.kdt.OrderTester 을 출력할때의 글자수를 의미하는데 logger{6}이라면 o.p.k.OrderTester 이런 식으로 프로그램내에서 글자수에 맞춰 줄여서 출력해준다. %msg%n : log message로그 Appender 설정 ConsoleAppender FileAppender RollingFileAppender앞서 사용했던 방식들이 ConsoleAppender 방식인데 이보다 FILEAppender를 사용해 file로써 log를 남기는 것이 더 좋다.FileAppender&lt;timestamp key=\"bySecond\" datePattern=\"yyyyMMdd'T'HHmmss\" /&gt;&lt;appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\"&gt; &lt;file&gt;log/kdt_${bySecond}.log&lt;/file&gt; &lt;append&gt;false&lt;/append&gt; &lt;encoder&gt; &lt;pattern&gt;${LOG_PATTERN}&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt;이런식으로 하면 log가 생성될때 마다 새로운 file로 생성이 된다.하지만 이렇게 할 경우 log파일이 무자비하게 생성된다는 문제점이 있어 하루에 하나씩 log file이 생성되게 하는 것이 일반적이다.이 때 사용하는 것이 RollingFileAppender이다.RollingFileAppender&lt;appender name=\"ROLLING_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;logs/access-%d{yyyy-MM-dd}.log&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;${LOG_PATTERN}&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt;&lt;logger name = \"org.prgrms.kdt\" level = \"debug\"&gt; &lt;appender-ref ref=\"ROLLING_FILE\"/&gt;&lt;/logger&gt;이렇게 작성하여 서버를 구동하게 되면 하루에 하나의 file이 만들어지고 그날의 log들이 그 파일에 기록되게 된다.Conversionconversion은 message format의 요소들의 색지정을 통해 log를 더 알아보기 쉽게 해주는 것이다.&lt;conversionRule conversionWord=\"clr\" converterClass=\"org.springframework.boot.logging.logback.ColorConverter\" /&gt;&lt;property name=\"CONSOLE_LOG_PATTERN\" value=\"%clr(%d{HH:mm:ss.SSS}){cyan} [%thread] %clr(%-5level) %logger{36} - %msg%n\" /&gt;&lt;property name=\"FILE_LOG_PATTERN\" value=\"%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n\" /&gt;이러한 방식으로 %clr를 통해 색을 지정해줄 수 있다.여기서 주의사항은 CONSOLE에 출력될때와 달리 FILE에 기록될때는 적용이 안되므로 위와 같이 CONSOLE_LOG_PATTERN과 FILE_LOG_PATTERN을 따로 설정해주는 것이 좋다." }, { "title": "Day14.Spring Framework (4)", "url": "/posts/SpringBootStart-day14/", "categories": "Backend, SpringBoot", "tags": "SpringBoot, Backend, SW", "date": "2022-07-29 14:00:00 +0900", "snippet": "[Day14] Spring Framework 핵심개념 (4)Application 속성관리를 spring에서 어떻게 하는지 중심적으로 학습하려고 한다.Environment profile예를 들어 개발시에는 H2 DataBase를 사용하도록 빈이 등록된다. 사용하는 DataSource의 Connection 대상이 H2 DataBase이 된다. 하지만 그 후 운영중에는 mysql을 사용하도록 바뀌게 된다면 DataBase, 즉 환경이 바뀌는 것이다.Properties Properties는 환경의 속성을 저장하는 것으로 이를 외부 속성으로 관리하고 application이 읽을 수 있게 한다.//application.propertiesversion = v1.0.0kdt.version = v1.0.0이렇게 properties를 정의한 후 OrderProperties에서 이 version을 불러와 확인해 보자. test class에서 bean을 가져와 불러오는 것이 아니라 InitializingBean을 사용해 class에서 바로 확인할 수 있다.@Componentpublic class OrderProperties implements InitializingBean { @Value(\"${kdt.version}\") // ${kdt.version:v0.0.0} 으로 kdt.version이 없을때 default값을 줄 수 있다. private String version; @Override public void afterPropertiesSet() throws Exception { System.out.println(MessageFormat.format(\"[OrderProperties] version -&gt; {0}\", version)); }}YAML로 property 작성 YAML의 정의는 “또 다른 마크업 언어 (Yet Another Markup Language)” 에서 유래되었다.//application.propertieskdt.version = v1.0.0kdt.minimum-order-amount = 1//application.yamlkdt: version: \"v1.0\" minimum-order-amount: 1properties에서 설정한 것을 yaml으로 바꾸면 이런 식으로 정의할 수 있다.@ConfigurationProperties 스프링부트에서 지원하는 기능으로 별도의 Bean을 만들 수 있게 도와준다. @ConfigurationProperties를 이용하면 특정 그룹의 속성을 모델링할 수 있고 Bean으로 등록해 사용할 수 있다. property들을 하나로 그룹화, 타입화를 시켜 다른 class들이 주입받아서 쓸 수 있도록 한다.스프링 Profile 애플리케이션 설정의 일부를 분리하여 특정 환경에서 사용할 수 있게 한다. 여러 Bean 정의들이 특정 Profile에서만 동작하게 할 수 있다. 특정 Property들을 특정 Profile로 정의해서 해당 Profile이 활동 중일때만 적용되게 할 수 있다.Resource 외부 resource (이미지, 텍스트, 암복호화를 위한 키파일) 들을 Resource, ResourceLoader 인터페이스를 제공해 하나의 API로 제공한다. 텍스트 파일 읽어오기 var resource2 = applicationContext.getResource(\"file:sample.txt\");var strings = Files.readAllLines(resource2.getFile().toPath());System.out.println(strings.stream().reduce(\"\",(a, b)-&gt; a + \"\\n\" + b)); url로 접근하여 읽어오기 var resource3 = applicationContext.getResource(\"https://stackoverflow.com/\");var readableByteChannel = Channels.newChannel(resource3.getURL().openStream());var bufferedReader = new BufferedReader(Channels.newReader(readableByteChannel, StandardCharsets.UTF_8));var contents = bufferedReader.lines().collect(Collectors.joining(\"\\n\"));System.out.println(contents); 이와 같이 텍스트든 url접근이든 applicationContext.getResource라는 일관된 방법으로 읽어와 처리할 수 있다." }, { "title": "Stop's Q2. Cloud", "url": "/posts/What-is-Cloud/", "categories": "Special Lecture, CLOIT", "tags": "Java, Spring", "date": "2022-07-28 15:00:00 +0900", "snippet": "Cloud 인터넷을 통해 액세스할 수 있는 서버와 이러한 서버에서 작동하는 소프트웨어와 데이터베이스를 의미한다. 클라우드 서버는 전 세계 데이터 센터에 위치한다. 사용자와 기업은 클라우드 컴퓨팅을 사용하면 직접 물리적 서버를 관리하거나 자체 서버에서 소프트웨어 응용 프로그램을 실행하지 않아도 된다.Cloud Computing 클라우드에서 서버, 스토리지, 소프트웨어 등 필요한 IT 자원을 받아서 사용하는 기술을 의미한다." }, { "title": "Stop's Q1. MessageFormat", "url": "/posts/stop'sQuestion/", "categories": "Stop's Study, Question", "tags": "Java, Spring", "date": "2022-07-28 15:00:00 +0900", "snippet": "MessageFormat" }, { "title": "Day12.Spring Framework (3)", "url": "/posts/SpringBootStart-day13/", "categories": "Backend, SpringBoot", "tags": "SpringBoot, Backend, SW", "date": "2022-07-25 15:00:00 +0900", "snippet": "[Day12] Spring Framework 핵심개념 (3)컴포넌트 스캔 컴포넌트 스캔이란 스프링이 직접 클래스를 검색해서 Bean으로 등록해주는 기능이다.설정 클래스, 즉 AppConfiguration에서 Bean으로 따로 등록하지 않아도 원하는 클래스를 빈으로 등록할 수 있다. 그 방법을 알아보자.1. 스트레오타입이는 번역하면 고정관념이라는 용어인데 UML 다이어그램을 확장시켜주는 도구로서 특정요소를 상황이나 도메인에 맞게 분류해주는 것이다.우리가 spring을 사용할때 클래스의 용도에 맞게 @Component, @Controller, @Service, @Configuration 등의 스트레오타입 애노테이션을 사용하여 컴포넌트 스캔을 하도록 할 수 있다.Autowired @Autowired는 Bean으로 등록이 된 상태에서 자동으로 의존성을 주입해주는 것을 말한다. 필요한 의존 객체의 타입에 해당하는 Bean을 찾아 주입하는 것이다.Autowired는 다음 3가지 경우에 사용할 수 있다. 생성자 setter 필드 @Autowired private VoucherRepository voucherRepository;// public VoucherService(VoucherRepository voucherRepository) {// this.voucherRepository = voucherRepository;// }보는 것과 같이 생성자를 지우고 필드에 Autowired 어노테이션을 추가하는 것으로 의존관계를 주입할 수 있다. @Autowired public VoucherService(VoucherRepository voucherRepository) { this.voucherRepository = voucherRepository; } public VoucherService(VoucherRepository voucherRepository, String dummy) { this.voucherRepository = voucherRepository; }다음 코드를 보면 생성자가 두개가 되는데 기본적으로 생성될 생성자에게 @Autowired를 붙인다. 생성자가 하나만 있는 경우에는 @Autowired 처리를 하지 않아도 자동으로 처리가 된다.보통 생성자에 Autowired를 사용하는 것을 권장하는데 이유는 다음과 같다. 초기화시에 필요한 모든 의존관계가 형성되기 때문에 안전하다. 잘못된 패턴을 찾을 수 있게 도와준다. 테스트를 쉽게 해준다. 불변성을 확보한다.그렇다면 자동으로 의존관계를 주입해줄때 Service에서 Repository를 생성하는데 이때 Repository가 두개 이상이라면 어떤 Repository를 생성해야하는지 모르게 되는 문제가 발생한다. 이때 어떻게 해야하는지 보자.실습중인 코드를 보면 VoucherSerive에서 VoucherRepository를 생성하는데 MemoryVoucherRepository와 jdbcVoucherRepository가 있다고 가정해보자. 그렇다면 이 VoucherRepository들을 구별시켜줘야하는데 방법은 다음과 같다.@Repository@Qualifier(\"memory\")public class MemoryVoucherRepository implements VoucherRepository { ...}@Repository@Qualifier(\"jdbc\")public class jdbcVoucherRepository implements VoucherRepository { ...}@Qualifier을 통해 Repository의 별칭을 정해주는 것이다. 그 후 Service의 생성자에서 이 별칭을 가지고 어떤 Repository를 기본적으로 불러올지 결정한다.public VoucherService(@Qualifier(\"memory\") VoucherRepository voucherRepository) { this.voucherRepository = voucherRepository;}또한 getBean을 할때에도// getBean을 통해 VoucherRepository를 불렀을때 VoucherRepositoy가 두개라 오류가 발생한다.var voucherRepository = applicationContext.getBean(VoucherRepository.class);// 이렇게 생성해주어야 한다.var voucherRepository = BeanFactoryAnnotationUtils.qualifiedBeanOfType(applicationContext.getBeanFactory(), VoucherRepository.class, \"memory\");아래와 같이 생성해주어야 하는데 특정 qualified된 Bean의 타입을 가지고 오는것으로 원하는 qualifier를 설정하여 불러올 수 있는 것이다.하지만 이런식으로 별칭을 주게 되면 사용자가 이 별칭들을 입력해줘야 하기 때문에 보통은 기본값으로 주어질 Repository에 @Primary를 주어 이 Repository를 기본 Repository로 설정하겠다 라는 설정을 합니다.Bean Scope Bean이 어떤 범위로 생성되는지를 말한다. singleton 기본적인 scpoe prototype request web 관련 session web 관련 application websocket" }, { "title": "Day12.Spring Framework (2)", "url": "/posts/SpringBootStart-day12/", "categories": "Backend, SpringBoot", "tags": "SpringBoot, Backend, SW", "date": "2022-07-20 19:30:00 +0900", "snippet": "[Day12] Spring Framework 핵심개념 (2)Inversion of Control (제어의 역전) : IoC 제어의 흐름이 역전되는 것객체가 자신이 사용할 객체를 스스로 생성, 선택하지 않는다.// 1. Order 스스로 Voucher를 결정하는 경우public class Order{ ... public Order(UUID orderid, UUID customerId, List&lt;OrderItem&gt; orderItems){ this.orderid = orderid; this.customerId = customerId; this.orderItems = orderItems; this.voucher = new FixedAmountVoucher(UUID.randomUUID(), 10L); }}// 2. 다른 class에서 어떤 Voucher를 사용할지 주입받는 IoC가 발생하는 경우public class Order{ ... public Order(UUID orderid, UUID customerId, List&lt;OrderItem&gt; orderItems, Voucher voucher){ this.orderid = orderid; this.customerId = customerId; this.orderItems = orderItems; this.voucher = voucher; }}public class OrderTester { public static void main(String[] args){ var fixedAmountVoucher = new FixedAmountvoucher(UUId.randomUUID(), 10L); Order(~ , fixedAmountVoucher); }}간단하게 코드로 표현해보았다.1번 코드의 경우 어떤 Voucher를 사용할지 Order class안에서 직접 결정하고 이는 Order가 직접 제어를 하고 있다고 말할 수있다.하지만 2번의 경우 Order에서 결정하는 것이 아니라 OrderTester에서 어떤 Voucher를 사용할지 결정해 Order는 어떤 Voucher를 사용할지 주입을 받게 된다. 이것이 바로 Order가 OrderTester에 의해 제어가 되고 있는 IoC상황인 것이다.실습에서는 그림과 같이 조금 더 발전해 OrderContext, 즉 IoC 컨테이너를 사용해 개별 객체들을 생성/파괴를 관장하도록 하는데 OrderService가 OrderRepository를 사용해 Order를 관리하긴 하지만 Interface로 만든 OrderRepository를 구체화하고 생성하는 역할은 OrderContext에서 처리하는 것이다.IoC 컨테이너가 Runtime의 의존성을 맺게 해줌으로써 클래스간의 느슨한 결합도를 가능하게 해주는 것이다. 핵심은 class가 자신이 사용할 객체를 스스로 만들지 않고 IoC 컨테이너를 통해 만들게 하여 단단한 결합도 생성을 방지한다.이런 IoC 컨테이너를 Spring에서는 ApplicationContext Interface를 통해 제공한다.ApplicationContextApplicationContext는 BeanFactory를 상속하는데 객체에 대한 생성/조합/의존관계설정 등을 제어하는 IoC의 기본기능을 BeanFactory가 담당하게 된다.BeanBean이란 IoC Container에 의해 관리되는 객체들을 말하는데 @Bean을 통해 Bean으로 만들어진다 라고 정의한다.Configuration Metadata스프링의 ApplicationContext는 실제 만들어야할 빈 정보를 Configuration Metadata로 부터 받아온다. 이 메타데이터를 이용해서 IoC 컨테이너에 의해 관리되는 객체들을 생성하고 구성한다. 객체들의 도면같은 느낌이라고 보면된다.AnnotationConfigApplicationContextJava 기반으로 설정을 할 때 이 구현체를 사용해 메타데이터를 설정하고 생성할 수 있다.Dependency InjectionIoC는 다양한 방법으로 만들 수 있다. 그 중 의존관계 주입패턴이 있는데 Order가 어떤 Voucher를 생성할지, OrderService가 어떤 orderRepository 객체를 생성할지 스스로 결정하는 것이 아니라 생성자를 통해 객체를 주입받는 패턴으로 이를 생성자 주입 패턴, Dependency Injection 이라고 한다." }, { "title": "Day11-2.Spring Framework (1)", "url": "/posts/SpringBootStart-day11/", "categories": "Backend, SpringBoot", "tags": "SpringBoot, Backend, SW", "date": "2022-07-19 13:28:00 +0900", "snippet": "[Day11-2] Spring Framework 핵심개념 (1)0. SpringBoot 사용법 Manual Setup : Maven / Gradle로 프로젝트 생성후 pom.xml / build.gradle을 직접 수정한다. Spring Boot : https://docs.spring.io/spring-boot/docs/2.5.0/reference/html/getting-started.html#getting-started Spring Boot CLI 설치 : https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started.html#getting-started.installing.cli 명령어 실행 : https://docs.spring.io/spring-boot/docs/current/reference/html/cli.html Spring initializ 이용 : Intellij의 Spring Initializr개인적으로 Spring Initializ가 가장 편했습니다.1. SpringBoot의 핵심개념 Spring IoC 컨테이너 / Beans 리소스 핸들링 (Resource와 ResourceLoader) 벨리데이션과 데이터 바인딩, 타입 변환 스프링 expression 언어 AOP Null-safety 데이터 버퍼와 코덱 로깅이 중 가장 핵심적인 내용 위주로 살펴보자.2. Domain Driven Design앞으로 스프링 부분 강의를 들으면서 주문관리 애플리케이션을 만들 것인데 이때 나오는 몇가지 용어들을 정리하고자 한다.DomainDomain은 비즈니스 그자체를 말하는데 예를들어 주문관리 애플리케이션에서는 주문에 관한 관리 자체가 Domain이 된다.여기서 자바기반으로 Domain Driven Design을 하게 되면 class로 작성하고 모두 객체로 만들어진다. 객체에는 종류가 있는데 Entity와 Value Object가 있다.Entity 앤터티는 다른 앤터티와 구별할 수 있는 식별자를 가지고 있고 시간에 흐름에 따라 지속적으로 변경되는 객체이다.다음 모델의 “주문” 객체는 “주문 Id”와 같은 식별자를 가지고 있고 지속적으로 변경되는 Entity이다.여기서 VO가 Value Object이다.Value Object 값 속성이 개별적으로 변화하지 않고 값 그 자체로 고유한 불변 객체이다.위에 그림에서 처럼 주문의 주문자를 보자. 이는 주문자의 이름이라는 string 값 자체로 고유한 불변 객체인 것이다.불변객체는 record를 사용하면 쉽게 만들 수 있다.식별자의 유무가 Entity와 Value Object의 차이가 아니다. Value Object도 식별자를 가질 수 있다. 지속적으로 변경하는지 불변객체인지가 핵심!!!3. 의존성 관리의존성 어떤 객체가 협력하기 위해 다른 객체를 필요로 할때 두 객체 사이의 의존성이 존재하게 된다.의존성은 실행 시점과 구현 시점에 서로 다른 의미를 가진다. 컴파일타임 의존성 : 코드를 작성하는 시점에서 발생하는 의존성 (클래스 사이의 의존성) 런타임 의존성 : 테스트할때 객체를 고르는 것으로 애플리케이션이 실행되는 시점의 의존성 (객체 사이의 의존성)여기서 Order는 FixedAmountVourcher에 의존하고 있기 때문에 FixedAmountVourcher에 수정사항이 생기면 Order에도 영향을 미친다. 이는 둘의 결합도가 강한 것으로 결합도를 느슨하게 해줄 필요가 있다.결합도 하나의 객체가 변경이 일어날 때에 관계를 맺고 있는 다른 객체에서 변화를 요구하는 정도 두 요소가 느슨한 결합도 / 약한 결합도를 가질때 두 요소 사이에 존재하는 의존성이 바람직하다고 한다. 반대로 두 요소가 강한 결합도를 가지면 두 요소 사이에 존재하는 의존성은 바람직하지 못한 것이다. 이렇게 하면 Order는 Voucher가 Percent이든 Fixed이든 관계없이 돌아갈 수 있다.이렇게 강한 결합도를 약한 결합도를 바꾸는게 바로 JavaFramework강의의 Interface부분에서 배웠던 부분이다. 참고) Day3.Interface다음시간에는 IoC, ApplicationContext, Dependency Injection에 관해 배워볼 예정이다." }, { "title": "Day11-1.Build Tool", "url": "/posts/BuildTool-day11/", "categories": "Backend, SpringBoot", "tags": "SpringBoot, Backend, SW, Gradle, Maven", "date": "2022-07-18 13:28:00 +0900", "snippet": "[Day11-1] Build ToolBuild 필요한 라이브러리를 다운 받고 classpath에 추가한다. 소스 코드를 컴파일 한다. 테스트를 실행한다. 컴파일된 코드를 packaging한다. -&gt; jar / war / zip etc packaging된 파일을 주로 artifacts 라고 부르고 server나 repository에 배포한다.Build Tool에는 대표적으로 Maven, Gradle이 있다.Maven 빌드 도구로써 주로 자바기반의 프로젝트에서 많이 사용한다.XML 기반으로 설정모델을 제공하고 pom.xml 파일로 작성할 수 있다.POM은 project object model의 약어 Maven을 사용하는 이유 archetypes라는 프로젝트 템플릿을 제공하여 반복되는 설정을 도와준다. 프로젝트에 사용하는 외부 라이브러리인 dependency를 관리해준다. 플러인과 외부 라이브러리를 분리하여 관리한다. dependency를 다운받는 Repository가 로컬이 될 수도 있고 Maven Central와 같은 공개된 Repository가 될 수도 있다. Maven 실습Intellij에서 프로젝트를 생성하면이렇게 Advenced Settings을 할 수있는데 이 부분을 Maven coordinates라고 한다.설정을 하고 나면 pom.xml에 나타나게 되는데 Maven coordinates는 프로젝트를 식별하는데 사용된다. groupId : 주로 회사, 단체명을 작성 artifactId : 주로 프로젝트명을 작성 version : 프로젝트의 버전 작성그리고 코드를 보면 &lt; modules &gt; 라는 부분이 있는데 이는 Maven의 Multiple Module 기능이다. Multiple Module 하나의 프로젝트에 여러 프로젝트를 관리할 수 있다. Transitive Dependencies Maven의 특이점 중 하나로 의존성의 의존성을 뜻한다.예를 들어 a가 b를 참조하고 b가 c를 참조할 때 a는 c를 transitive 의존성으로 간주하는 것이다. 이렇게 되면 의존성 트리가 구성되고 동일한 groupId, artifactId에 대해서 가장 최신의 version을 가져와 사용한다. 좋은 기능같아 보이지만 버전, 라이브러리 간의 충돌 등의 문제가 야기될 수 있다는 단점이 있다.Gradle Groovy / 코틀린 기반으로 스크립트를 작성하게 도와 주는 Build ToolGradle도 intellij서 사용해보면 다음과 같이 build.gradle.kts를 확인할 수 있다.Project &amp; Task Gradle은 하나 이상의 프로젝트를 지원한다. 이는 마치 Maven의 Multiple Module과 비슷하다. 그리고 하나의 프로젝트는 하나 이상의 Task로 구성된다. Task는 클래스를 컴파일하거나 Jar를 생성하거나 하는 등의 build를 위한 작업이다. 일반적으로 Task는 Plugin에 의해서 제공된다.Plugin Gradle의 실제 Task와 주요한 기능들을 추가하게 해주는 것이다. 하나의 플로젝트의 여러 플러그인을 추가할 수 있다. 플러그인을 추가하면 새로운 Task들이 추가되고 도메인 객체나 특정 컨벤션들이 추가된다. Maven VS Gradle Build Tool을 처음 접할 때는 Gradle을 공부해서 사용하는 것을 추천한다. 회사에서 Maven을 사용한다면 Maven을 학습하자. Maven은 XML 설정이 장황하지만 별다른 학습없이 직관적으로 사용할 수 있기 때문에 간단한 프로젝트를 만들때에도 Maven을 권장한다." }, { "title": "Day8-2.MySQL 실습 (GROUP BY 편)", "url": "/posts/MySQL-execrcise2-day8/", "categories": "Backend, Database", "tags": "Database, Backend, SW, MySQL", "date": "2022-07-15 14:24:00 +0900", "snippet": "[DAY8] MySQL 실습 (2)GROUP BY SELECT 실행시 결과를 특정 그룹별로 묶는 방법GROUP BY &amp; Aggregate 함수 테이블의 레코드를 그룹핑하여 그룹별로 다양한 정보를 계산하는 것이다. 두 단계로 이루어진다. 그룹핑 할 필드를 결정한다. (하나 이상) 필드이름이나 필드 일련번호를 사용해 GROUP BY로 지정한다. 다음 그룹별로 계산할 내용을 결정한다. 여기서 Aggregate함수를 사용한다. COUNT, SUM, AVG, MIN, MAX, GROUP_CONCAT … 보통 필드 이름을 지정하는 것이 일반적이다. (alias) GROUP BY부터는 MySQL Workbench를 통해 문제를 풀어보며 공부하고자 한다.1. 월별 세션수를 계산하라 prod.session을 사용한다. (id, created 필드) SELECT LEFT(created, 7) AS mon, -- created의 왼쪽부터 7글자를 이제 mon이라 부르겠다. COUNT(1) AS session_countFROM prod.sessionGROUP BY 1 -- = GROUP BY mon, GROUP BY LEFT(created, 7)ORDER BY 1; GROUP BY 1 이라는 것은 SELECT문의 첫번째 줄을 그룹화 하겠다는 뜻이다. 따라서 월별로 session_count가 되어 필드값이 나오는 것을 확인할 수있다.2. 가장 많이 사용된 채널은 무엇인가?SELECT\tchannel_id, COUNT(1) AS session_count, COUNT(DISTINCT user_id) AS user_countFROM sessionGROUP BY 1ORDER BY 2 DESC;channel_id를 그룹화하여 count를 하게 되는데 두가지 경우가 있다. 첫번째는 그저 채널의 사용수를 구하는 것이고 두번째는 한사람이 채널을 두번 사용하는 중복을 없애고 채널을 사용한 사용자 수를 구하는 것이다.ORDER BY 2 를 하느냐, 3을 하느냐로 어떤요소의 내림차순으로 필드를 보여줄지 결정할 수 있다. 사진은 ORDER BY 2 DESC 를 한 결과이기에 session_count가 내림차순으로 보여진다.3. 가장 많은 세션을 만들어낸 사용자 ID는 무엇인가?SELECT\tuser_id, COUNT(1) user_idFROM sessionGROUP BY 1ORDER BY 2 DESCLIMIT 1;이 문제를 해결하기 위해서는 우선 사용자 별로 얼마나 session을 이용했는지 count하고 이를 내림차순으로 정렬한다. 그 후 가장 많은 ~ 이므로 LIMIT 1 을 하여 결과를 도출한다.4. 월별 채널별 유니크한 사용자 수 channel_id를 channel로 바꾸어 표현 -&gt; JOIN이 문제는 우선 둘의 공통필드은 channel_id를 이용해 JOIN을 해야한다. JOIN하는 법은 다음과 같다.SELECT s.id, s.user_id, s.created, s.channel_id, c.channel FROM session s\t-- session을 s로 표현하겠다.JOIN channel c ON c.id = s.channel_id;이러한 JOIN을 이용해 문제를 풀어보자.SELECT \tLEFT(created,7) AS mon, c.channel, COUNT(distinct user_id) AS user_countFROM session sJOIN channel c ON c.id = s.channel_idGROUP BY 1,2ORDER BY 1 DESC, 2;\t-- 1을 내림차순으로 하되 같은 1안에서는 2를 오름차순으로이렇게 JOIN 후 GROUP BY를 사용하면 두 클래스의 중복 값을 이용해 필드를 사용할 수 있다." }, { "title": "Day8-1.MySQL 실습 (SELECT 편)", "url": "/posts/MySQL-exercise1-day8/", "categories": "Backend, Database", "tags": "Database, Backend, SW, MySQL", "date": "2022-07-14 14:54:00 +0900", "snippet": "[DAY8] MySQL 실습 (1)SELECTchannel class id channel 1 Instagram 2 Naver 3 Youtube 4 Google 5 Facebook 6 Tiktok 7 Unknown session class id user_id created channel_id 1 779 2019-05-01 00:36:00 5 2 230 2019-05-01 02:53:49 3 3 369 2019-05-01 12:18:27 6 4 248 2019-05-01 13:41:29 5 5 676 2019-05-01 14:17:54 2 6 40 2019-05-01 14:42:50 3 7 468 2019-05-01 15:08:16 1 8 69 2019-05-01 15:20:27 4 다음의 두 클래스를 예제로 실습을 진행해 보려고 한다.SELECT 테이블에서 레코드들을 읽어오는데 사용한다.WHERE을 사용해 조건을 만족하는 레코드이다.SELECT 필드이름1, 필드이름2, ...FROM 테이블이름WHERE 선택조건GROUP BY 필드이름1, 필드이름2, ...ORDER BY 필드이름 [ASC|DESC] -- 필드 이름 대신에 숫자 사용가능LIMIT N;와 같이 사용한다. 몇가지 예시를 보자.“session class 모든 것을 보여달라”SELECT * -- *는 모든 필드를 지칭하는 표현FROM prod.session; -- 앞서 USER prod;를 수행했다면 FROM session도 사용 가능“session class에서 id, user_id, channel_id만 10개 보여달라”SELECT id, user_id, channel_idFROM prod.sessionLIMIT 10;“session class에서 channel_id 값들의 요소들을 보여라. (중복 X)”SELECT DISTINCT channel_id --유일한 channel_id를 알고 싶은 경우FROM prod.session;“prod.session 의 레코드 수를 하나씩 count해라”SELECT COUNT(1) -- 테이블의 모든 레코드 수를 카운트, COUNT(*)FROM prod.session;“channel_id가 5인 경우만 count해라”SELECT COUNT(1)FROM prod.sessionWHERE channel_id = 5;CASE WHEN 필드 값의 변환을 위해 사용 가능 CASE WHEN 조건 THEN 참일때 값 ELSE 거짓일때 값 END 필드이름 여러 조건을 사용하여 변환하는 것도 가능 CASE WHEN 조건1 THEN 값1 WHEN 조건2 THEN 값2 ELSE 값3END 필드이름EX.SELECT channel_id, CASE WHEN channel_id in (1, 5, 6) THEN 'Social-Media' WHEN channel_id in (2, 4) THEN 'Search-Engine' ELSE 'Something-Else'END channel_typeFROM prod.session;예시의 경우 “chaneel_id필드의 1, 5, 6은 ‘Social-Media’로 2, 4는 ‘Search-Engine’으로, 나머지는 ‘Something-Else’로 이름을 바꾸고 필드이름도 channel_type로 변경해라” 는 뜻입니다.따라서 제일 처음 나왔던 session 테이블 사진에서 channel_id 필드이름은 channel_type으로 변경되고 값들은 대응되는 type이름으로 변경되게 됩니다.NULL 값이 존재하지 않음을 나타내는 상수0혹은 ““과는 다르다. 필드 지정시 값이 없는 경우 NULL로 지정이 가능하다. 테이블 정의시 디폴트 값으로도 지정 가능 어떤 필드의 값이 NULL인지 아닌지 비교는 특수한 문법으로 한다. field1 is NULL field1 is not NULL NULL이 사칙연산에 사용된다면 SELECT 0 + NULL 0 - NULL 0 * NULL 0/NULL COUNTCOUNT()는 괄호안의 NULL이 아니면 하나씩 카운트 하고 NULL이면 카운트를 안한다. COUNT함수는 예시를 통해 알아보자. value NULL 1 1 0 0 4 3 다음과 같은 prod.count_test가 있다. SELECT COUNT(1) FROM prod.count_test -&gt; 7 SELECT COUNT(0) FROM prod.count_test -&gt; 7 SELECT COUNT(NULL) FROM prod.count_test -&gt; 0 SELECT COUNT(value) FROM prod.count_test -&gt; 6 SELECT COUNT(DISTINCT value) FROM prod.count_test -&gt; 4value일 경우는 NULL이 아닌 값만 카운트 한다.WHEREIN WHERE channel_id in (3, 4) : channel_id 의 3,4 WHERE channel_id = 3 OR channel_id = 4 NOT IN : ~ 빼고 다 EX. “channel_id가 4, 5인 것만 COUNT해라”SELECT COUNT(1)FROM prod.sessionWHERE channel_id IN (4, 5);LIKE LIKE : 대소문자 구별없이 문자열 매칭 기능 제공 WHERE channel LIKE ‘G%’ -&gt; ‘G*’ WHERE channel LIKE ‘%o%’ -&gt; ‘* o *’ NOR LIKE EX. “channel의 중간에 G가 들어가는 것을 COUNT해라”“” SELECT COUNT(1)FROM prod.cannelWHERE channel LIKE '%G%'; STRING Functions LEFT(str, N) : str을 왼쪽에서 부터 N만큼 REPLACE(str, exp1, exp2) : str의 exp1을 exp2로 대체 UPPER(str) : str을 모두 대문자로 바꾸기 LOWER(str) : str을 모두 소문자로 바꾸기 LENGTH(str) : str의 길이 LPAD, RPAD SUBSTRING : str 특정부분 추출 CONCATORDER BY 디폴트 순서는 오름차순 (작은 값부터) ORDER BY 1 ASC 내림차순(Descending)을 원하면 “DESC” ORDER BY 1 DESC 여러개의 필드를 사용해서 정렬하려면 ORDER BY 1 DESC, 2, 3 NULL 값 순서는? 오름차순 일 경우, 처음 내림차순 일 경우, 마지막 SELECT valueFROM prod.count_testORDER BY value DESC / ASC ;타입 변환 DATE Conversion NOW : 현재시각 return 타임존 관련 변환 CONVERT_TZ(now(), ‘GMT’, ‘Asia/Seoul’) DATE, WEEK, MONTH, YEAR, HOUR, MINUTE, SECOND, QUARTER, MONTHNAME DATEDIFF : 날 사이의 차이 (GAP) DATE_ADD : N일뒤의 날짜 return STR_TO_DATE DATE_FORMAT예를 몇가지 보자. created 2019-01-01 00:06:48 이런 class를 다음 형변환들로 필드를 추가할 수 있다.SELECT created, CONVERT_TZ(created, 'GMT', 'Asia/Seoul') seoul_time, YEAR(created) y, QUARTER(created) q, MONTH(crated) m, MONTHNAME(created) mnn, DATE(created) d, HOUR(created) h, MINUTE(created) m, SECOND(created) s from session LIMIT 10;이렇게 되면 created seoul_time y q m mn d h m s 2019-01-01 00:06:48 2019-01-01 09:06:48 2019 1 1 January 2019-01-01 0 6 48 이렇게 형변환이 되어 필드값에 저장되는 것을 볼 수있다.다음은 현재시점에서 created와의 gap을 계산해주는 예시이다.SELECT created, DATEDIFF(now(), created) gap_in_days, DATE_ADD(created, INTERVAL 10 DAY) ten_days_after_createdFROM sessionLIMIT 10;|created|gap_in_days|ten_days_after_created||:-|:-|:-||2019-01-01 00:06:48|936|2019-01-11 00:06:48|gap_in_days는 현재와 created와의 날짜 차이를, ten_days_after_created는 created로 부터 10일 뒤의 날을 필드에 저장해 보여주고 있다.Type Casting 1/2의 결과 0이다. 정수간의 연산은 정수가 되어야 하기 때문이다. 분자나 분모 중의 하나를 float로 캐스팅해야 0.5가 나온다. 이는 프로그래밍 언어에서도 일반적으로 동일하게 동작한다. cast 함수를 사용한다. cast (category as float) convert (expression, float) 문자열을 float로 SELECT cast(‘100.0’ as float) SELECT convert(‘100.0’, float); " }, { "title": "Day7.MySQL", "url": "/posts/MySQL-day7/", "categories": "Backend, Database", "tags": "Database, Backend, SW, SQL", "date": "2022-07-13 13:45:00 +0900", "snippet": "[DAY7] MySQL1. MySQLMySQL의 역사 1995년, 스웨덴 회사였던 MySQL AB에 의해 개발된 관계형 데이터베이스 시작은 오픈소스였다. My는 개발자 중 한 사람의 딸 이름 이었다. 2008년, 썬 마이크로시스템이 MySQL AB를 $1B을 주고 인수했다. 2009년, 오라클이 썬을 인수하면서 MySQL의 유료화 여부가 쟁접이 되었다. 2010년 MySQL의 처음 개발자였던 Monthy가 MySQL과 호환이 되는 MariaDB라는 오픈소스가 개발이 되었다.MySQL의 종류와 버전 MariaDB 오픈소스로 무료이다. MySQL 5.5에 기반해서 개발되었다. MySQL과 인터페이스는 동일하나 성능은 더 좋다. MySQL 두가지 종류가 존재한다. MySQL Community Server : 오픈소스로 무료이다. 앞으로 사용해 공부할 예정이다. MySQL Enterprise Server : 유료버전으로 다양한 플러그인이 제공된다. 현재기준 8.0이 최신버전이다. MySQL의 특징 한동안은 웹개발 표준 기술 스택 중의 하니였다. LAMP : Linux, Apache, MySQL, PHP 지금도 Postgress와 함께 가장 널리 쓰이는 프로덕션용 관계형 데이터베이스이다. 용량 증대 (Scaling) 방식 Scale-Up : 서버에 CPCU와 Memory 추가 Sacle-Out : Master-Slave 구성으로 클러스터 구성을 이야기하지만 MySQL은 이를 지원하지 못한다. 2. 클라우드클라우드의 정의 컴퓨팅 자원 (하드웨어, 소프트웨어 등등)을 네트워크를 통해 서비스 형태로 제공하는 것이다. 키워드 : “No Provisioning”, “Pay As you Go” 자원(ex. 서버)을 필요한 만큼 실시간으로 할당하여 지불한다.(* 탄력적으로 필요한 만큼의 자원을 유지하는 것이 중요하다.)클라우드 컴퓨팅이 없었다면? 서버/네트워크/스토리지 구매와 설정 등을 직접 수행해야 한다. 데이터셍터 공간을 직접 확보해야한다. (Co-location) 확징이 필요한 경우 공간을 먼저 더 확보해야 한다. 그 공간에 서버를 구매하여 설치하고 네트워크를 설정한다. 보통 서버를 구매해서 설치하는데 적어도 두세달은 걸린다. 또한 Peak time을 기준으로 Capacity planning을 해야한다. 놀고 있는 자원들이 높게 되는 현상이 발생한다. 직접 운영비용 vs 클라우드비용 : 기회비용!클라우드 컴퓨팅의 장점 초기 투자 비용이 크게 줄어든다. CAPEX (Capital Expenditure) VS OPEX (Operating Expense) 리소스 준비를 위한 대기시간이 감소한다. Shorter Time to Market 노는 리소스 제거로 비용이 감소한다. 글로벌 확장이 용이하다. 소프트웨어 개발시간이 단축된다. Managed Servece (Saas) 이용 3. AWS 가장 큰 클라우드 컴퓨팅 서비스 업체 2002년, 아마존의 상품데이터를 API로 제공하면서 시작되었다. 현재 100여개의 서비스를 전세계 15개의 지역에서 제공하고 있다. 대부분의 서비스들이 오픈소스 프로젝트들을 기반으로 한다. 최근 들어 ML/AI 관련 서비스들도 내놓기 시작했다. 사용대상 다수의 상장업체들과 많은 국내 업체들도 사용을 시작하였다. 다양한 종류의 소프트웨어/플랫폼 서비스를 제공한다. AWS의 서비스만으로 쉽게 온라인서비스를 생성할 수 있다. EC2 : Elastic Cloud Compute AWS의 서버 호스팅 서비스로 다야한 종류의 서버 타입을 제공한다. 리눅스나 윈도우 서버를 launch하고 로그인이 가능하다. (구글앱 엔진과의 가장 큰 차이) 가상 서버들이라 전용서버에 비해 성능이 떨어진다. Bare-metal 서버도 제공하기 시작했다. http://aws.amazon.com/ec2/ 구매옵션 On-Demand : 시간당 비용을 지불하는 것 (가장 흔하다.) Reserved : 1년, 3년간 사용을 보장하고 1/3 정도에서 40% 할인을 받는 옵션 Spot Instance : 일종의 경매방식으로 놀고 있는 리소스들을 보다 싼 비용으로 사용할 수 있는 옵션 (내가 사용하다가 더 높은 가격으로 누군가가 사용하면 사용할 수 없다. 임시방편으로 싸게 사용하는 용도) S3 : Simple Storage Servicㄷ 아마존이 제공하는 대용량 클라우드 스토리지 서비스 http://aws.amazon.com/s3/ S3은 데이터 저장관리를 위해 계층적으로 구조를 제공한다. 글로벌 네임스페이스를 제공하기 때문에 톱레벨 디렉토리 이름 선정에 주의해야한다. S3에서는 디렉토리를 Bucket이라 부른다. Bucket이나 파일별로 액세스 컨트롤이 가능하다.AWS RDS AWS가 제공해주는 다양한 관계형 데이터베이스 서비스" }, { "title": "Day6-2.SQL", "url": "/posts/SQL-day6/", "categories": "Backend, Database", "tags": "Database, Backend, SW, SQL", "date": "2022-07-12 16:40:00 +0900", "snippet": "[DAY6-2] SQL0. SQL 이란? SQL : Structured Query Language 관계형 데이터베이스에 있는 데이터(테이블)를 질의하거나 조작해주는 언어 1970년대 초반에 IBM이 개발한 구조화된 데이터 질의 언어이다. 두 종류의 언어로 구성 DDL(Data Definition Language):테이블의 구조를 정의하는 언어 DML(Data Manipulation Language):테이블에서 원하는 레코드를 읽어오고 레코드를 추가/삭제/갱신해주는데 사용하는 언어** 이러한 SQL은 빅데이터 세상에서 중요하다 구조화된 데이터를 다루는 한 SQL은 데이터 규모와 상관없이 쓰인다. 모든 대용량 데이터 웨어하우스는 SQL기반이다. ex. Redshift, Snowflake, BigQuery, Hive Spark나 Hadoop도 SparkSQL과 Hive라는 SQL언어가 지원된다. 백엔드/프론트엔드/데이터 분야 모두에서 반드시 필요한 기본 기술이다.1. SQL의 단점 구조화된 데이터를 다루는데 최적화가 되어있다. (= 비구조화된 데이터를 다루기 어렵다.) 정규표현식을 통해 비구조화된 데이터를 어느 정도 다루는 것은 가능하나 제약이 심하다. 많은 관계형 데이터베이스들이 플랫한 구조만 지원한다. (no nested like JSON) 구글 BigQuery는 nested structure를 지원한다. 비구조화된 데이터를 다루는데 Spark, Hadoop과 같은 분산 컴퓨팅 환경이 필요해졌다. (즉 SQL만으로는 비구조화 데이터를 처리하지 못한다.) 관계형 데이터베이스마다 SQL문법이 조금씩 상이하다.2. schema의 종류Star schema Production DB용 관계형 데이터베이스에서 보통 사용한다. 데이터를 논리적 단위로 나눠 저장하고 필요시 조인한다. 스토리지 낭비가 덜하고 업데이트가 쉽다.Denormalized schema NoSQL이나 데이터웨어하우스에서 사용하는 방식 단위 테이블로 나눠 저장하지 않음으로 별도의 조인이 필요없는 형태 Star schema 보다 스토리지를 더 사용하지만 조인이 필요없기 때문에 더 빠른 계산이 가능하다.3. SQL의 기본 다수의 SQL문은 세미콜론으로 분리한다.ex. SQL문1; SQL문2; SQL문3; SQL 주석 ”–” : 인라인 한줄짜리 주석 (자바의 //) ”/* – */ : 여러줄에 걸쳐 사용 가능한 주석 SQL 키워드는 대문자를 사용한다던지 하는 나름의 포맷팅이 필요하다.ex. 팀 프로젝트라면 팀에서 사용하는 공통 포맷이 필요 테이블 / 필드이름의 명명규칙을 정하는 것이 중요하다. 단수형 vs 복수형 : User vs Users _ vs CamelCasing : user_session_channel vs UserSessionChannel 4. 테이블 필드의 중요 속성PRIMARY KEY 테이블에서 레코드의 유일성을 정의하는 필드이다.ex. 사용자 테이블에서 이메일, 주민등록번호 Composite primary key : primary key가 두개이상 필드로 정의되는 경우 Primary key로 지정된 필드가 있는 경우 데이터베이스단에서 중복된 값을 갖는 레코드가 생기는 것을 방지할 수 있다. primary key uniqueness constraint Foreign key 테이블의 특정 필드가 다른 테이블의 필드에서 오는 값을 갖는 경우NOT NULL 필드의 값이 항상 존재해야하는 경우DEFAULT value 필드에 값이 주어지지 않은 경우 기본값을 정의 해주는 역할 timestamp 타입 : CURRENT_TIMESTAMP를 사용하면 현재 시간으로 설정된다.5. SQL DDL (테이블 구조 정의 언어)CREATE TABLE table 생성 Primary key 속성을 지정할 수 있다.(Primary key uniqueness : 유일키를 보장하여 동일한 키가 존재하지 않는다.) 성능향상을 위해 인덱스를 지정할 수 있다.CREATE TABLE raw_data.user_session_channel ( userid int, sessionid varchar(32) primary key, channel varchar(32));DROP TABLE table 삭제 DROP TABLE table_name; 없는 테이블을 지우려고 할땐 에러가 난다. DROP TABLE IF EXISTS table_name; vs. DELETE FROM : 조건에 맞는 레코드들만 지운다.ALTER TABLE 새로운 컬럼 추가 / 기존 컬럼 이름 변경 / 기존 컬럼 제거 / 테이블 이름 변경 새로운 컬럼 추가ALTER TABLE 테이블이름 ADD COLUMN 필드이름 필드타입; 기존 컬럼 이름 변경ALTER TABLE 테이블이름 RENAME 현재필드이름 to 새필드이름; 기존 컬럼 제거ALTER TABLE 테이블이름 DROP COLUNM 필드이름; 테이블 이름 변경ALTER TABLE 현재테이블이름 RENAME to 새테이블이름;SELECT 레코드 질의 언어(앞으로 자세히 공부하며 포스팅할 예정입니다.) SELECT FROM : 테이블에서 레코드와 필드를 읽어오는데 사용한다. WHERE를 사용해서 레코드 선택조건을 지정한다. GROUP BY를 통해 정보를 그룹 레벨에서 뽑는데 사용하기도 한다. DAU, WAU, MAU 계산은 GROUP BY를 필요로 한다. ORDER BY를 사용해서 레코드 순서를 결정하기도 한다. 보통 다수의 테이블에 조인해서 사용한다.그 밖의 키워드 레코드 추가 : INSERT INTO 레코드값 수정 : UPDATE FROM 레코드 삭제 : DELETE FROM (vs. TRUNCATE)" }, { "title": "Day6-1.Database", "url": "/posts/Database-day6/", "categories": "Backend, Database", "tags": "Database, Backend, SW", "date": "2022-07-11 18:40:00 +0900", "snippet": "[DAY6-1] Database1. 데이터베이스는 왜 필요한가? 모든 서비스는 데이터를 만들어내고 그 데이터는 기록이 되어야 한다. 우리가 어떤 사이트에 회원 가입을 하게되면 회원정보 (ID, 암호, 전화번호, 이름) 등의 데이터가 생성되고 이는 저장된다. 이와 같이 저장된 다양한 데이터들이 머신러닝, 인공지능 등을 통해 분석되어 사용자에게 맞춰진 기능제공이 가능한 것이다.그럼 이러한 데이터들은 어디에 저장이 될까?프로덕션 관계형 데이터 베이스 : RDBMS 관계형 데이터 베이스 중에서 웹서비스, 앱의 운영에 필요한 정보들을 저장해주는 데이터 베이스ex. MySQL, PstreSQL 빠른 처리속도가 중요하다. 데이터 베이스의 처리속도가 곧 웹,앱의 동작속도이기 때문이다.(VS. 데이터 웨어하우스 관계형 데이터베이스) 백엔드 개발자면 기본적으로 알아야하는 기술이다. SQL은 관계형 데이터베이스의 프로그래밍 언어이다.이렇게 저장된 데이터를 분석하기 위해서 필요한 것이 데이터웨어하우스이다.데이터웨어하우스 데이터 분석을 위한 데이터베이스ex. BigQuery, Snowflake, MySQL 회사 관련 데이터를 저장하고 분석함으로써 의사결정과 서비스 최적화에 사용한다. 처리속도보다는 구조화 된 큰 데이터를 처리하는 것이 중요하다.(VS. 프로덕션 관계형 데이터베이스) 데이터직군이라면 반드시 알아야한다. (SQL)데이터 순환 구조 프로덕션용 데이터베이스와 데이터 웨어하우스데이터베이스의 종류 관계형 데이터베이스 : 구조화된 데이터 (table 형태) 프로덕션용 관계형 데이터베이스 데이터 웨어하우스용 관계형 데이터베이스 비관계형 데이터베이스 : 비구조화 데이터도 다룸 흔히 NoSQL 데이터베이스라고 부르기도 한다. 보통 프로덕션용 관계형 데이터베이스를 보완하기위한 용도로 많이 사용한다. 크게 4종류가 존재한다. Key/Value Storage : Redis, Memcache, … Document Store : MongoDB Wide Column Storage : Cassandra, HBase, DynamoDB Search Engine : Elastic Search 2. 백엔드 시스템 구성도 예제어떤 개발자 직군이 있는지 알아보고 관계형 데이터 베이스가 전체 시스템에서 어떻게 사용되는지 구성도를 살펴보자.프론트엔드 &amp; 백엔드 초기에는 웹/앱 서비스를 이 둘로 나뉘었다. 프론트엔드 사용자와 인터랙션을 하는 부분이다. 보통 웹 브라우저나 모바일 폰에 사용자에게 노출되는 서비스를 말한다. 백엔드 프론트엔드 뒤에 숨어서 사용자에게 보이지 않는 실제 데이터를 저장/추가하고 사용자가 요구한 일을 수행하는 부분이다. 여기서 다양한 데이터베이스들이 사용된다. 다른 직군의 등장 데브옵스 (DevOps) 주로 백엔드에 집중을 두고 서비스의 운영을 책임지는 팀이다. 회사가 작을때는 보통 백엔드 팀이 이 일을 담당한다. 풀스택 (FullStack) 개발속도롤 내기위해 프론트엔드/백엔드를 모두 할 수 있는 개발자를 말한다. 보통 작은 회사에서 선호하는 형태의 직군이다. 데이터 직군 : 사실상 소프트웨어 개발자로 데이터 웨어하우스 관련일을 담당한다. 데이터 엔지니어 : 소프트웨어 개발자로 데이터 웨어하우스와 관련일을 담당한다. (MLOps라는 직군이 나타나기 시작한다.) 데이터 분석가 : 데이터 웨어하우스를 기반으로 다양한 지표설정과 분석을 수행한다. 데이터 과학자 : 수집된 과거 데이터를 기반으로 미래를 예측하는 모델링 혹은 개인화작업으로 서비스의 만족도를 높이고 프로세스의 최적화를 수행한다. 시스템 구성의 변화2 tier 보통 데스크탑 응용프로그램에서 사용되는 아키텍쳐 클라이언트와 서버 두개의 티어로 구성된다. 클라이언트 : 사용자가 사용하는 UI (front-end) 서버 : 데이터베이스 (back-end) 3 tier 웹 서비스에서 많이 사용되는 아키텍쳐 프리젠테이션 티어 : front-end 애플리케이션 티어 : back-end 데이터 티어 : back-end 앞으로 배울 Spring Boot Spring Boot : 자바기반의 백엔드 프레임워크 중의 하나 기본적 3 tier 구조 프론트엔드는 보통 React 3 tier의 프리제네이션, 애플리케이션 티어의 구현을 쉽게 한다.관계형 데이터베이스의 중요성 어떤 구조이건 데이터베이스는 꼭 필요한 컴포넌트이다. 이 데이터베이스를 잘 다루는 것이 좋은 개발자가 되기 위해 필요하다.(기본은 SQL을 잘 아는 것) 백엔드 개발자로써 중요한 부분이다. 데이터 모델을 잘 만들고 그걸 프론트 개발자와 공유하고 협업해야한다. 속도 개선을 위한 쿼리 성능을 모니터링하고 필요시 성능개선을 수행한다.(어떤 경우에는 이를 전담하는 사람이 존재한다.) 그럼 관계형 데이터베이스가 무엇인지 자세히 알아보자.3. 관계형 데이터베이스 구조화된 데이터를 저장하고 질의할 수 있도록 해주는 스토리지 엑셀 스프레드시트 형태의 테이블로 데이터를 정의하고 저장한다. 테이블에는 컬럼(열)과 레코드(행)이 존재 관계형 데이터베이스를 조작하는 프로그래밍 언어 : SQL 테이블 정의를 위한 DDL (Data Definition Language) 테이블의 포맷을 정의해주는 언어 테이블 데이터 조작/질의를 위한 DML (Data Manipulation Language) DDL로 정의된 테이블에 레코드를 추가, 수정, 삭제 혹은 읽어들이기 위해 사용하는 언어 대표적 관계형 데이터베이스 프로덕션 데이터베이스 : MySQL, PostgreSQL, Oracle, … OLTP (OnLine Transaction Processing) 빠른 속도가 중요하다. 서비스에 필요한 정보 저장 데이터 웨어하우스 : Redshift, Snowflake, BigQuery, Hive, … OLAP (OnLine Analytical Processing) 처리 데이터 크기에 집중한다. 데이터 분석이나 모델 빌딩 등을 위한 데이터를 저장한다. 보통 프로덕션 데이터베이스를 복사해서 데이터 웨어하우스에 저장한다.관계형 데이터베이스의 구조 관계형 데이터베이스는 2단계로 구성된다. 가장 밑단에는 테이블들이 존재한다. (테이블은 엑셀의 시트에 해당) 테이블들은 데이터베이스(or 스키마)라는 폴더 밑으로 구선된다. (엑셀에서는 파일) 테이블의 구조 (= 테이블 스키마) 테이블은 레코드들로 구성된다. (= 행) 레코드는 하나 이상의 필드(컬럼)으로 구성된다. (= 열) 필드(컬럼)는 이름과 타입과 속성(primary key)로 구성된다. " }, { "title": "Day4.Collection", "url": "/posts/Java-for-framework-day4/", "categories": "Backend, Java for framework", "tags": "Java, Backend, SW", "date": "2022-07-07 16:26:00 +0900", "snippet": "[DAY4] CollectionCollection Collection은 여러 데이터의 묶음이다. (묶음 단위로 움직인다.) 추상체 이다.Iterator 여러 데이터의 묶음(Collection)을 풀어서 하나씩 처리할 수 있는 수단을 제공한다. next()를 통해서 다음 데이터를 조회할 수 있다. (이전 데이터는 조회불가) List&lt;String&gt; list = Arrays.asList(\"A\", \"BC\", \"DEF\");Iterator&lt;String&gt; iter = list.iterator();while(iter.hasNext()) { System.out.println(iter.next());}이처럼 list를 interator()로 설정하면 iter.hasNext()로 다음 데이터가 있는지 확인하고 iter.next()로 값들을 불러올 수 있다.Stream 데이터의 연속 이다. Java 8 이상에서 부터 사용가능하다. 이미 우리가 쓰고 있는 Stream에는 System.in / System.out 가 있다. Java 8 : Collection.stream() 을 제공한다. filter / map / forEach 등과 같은 고차함수 (함수형 인터페이스를 사용하여 함수를 인자로 받는 함수)를 제공한다. public class Main { public static void main(String[] args) { Arrays.asList(\"A\", \"AB\", \"ABC\", \"ABCD\", \"ABCDE\") .stream() .map(s -&gt; s.length()) //.map(String::length) .filter(i -&gt; i % 2 == 1) .forEach(i -&gt; System.out.println(i)); //.forEach(System.out::println) }}다음과 같이 Stream을 사용할 수 있다. 이 밖에도 count, reduce 등 다양한 고차함수로 기능들을 제공한다.(주석은 method reference로 바꾼 코드로 같은 의미이다.) 스트림을 만들때는 Stream.generate / Stream.iterate 로 만들 수 있다.이를 사용해 random한 숫자를 10개 생성하는 코드예시이다.//Stream.generate 사용예시Random r = new Random();Stream.generate(r::nextInt) //Stream.generate(() -&gt; r.nextInt()) .limit(10) .forEach(System.out::println)//Stream.iterate 사용예시Stream.iterate(seed:0, (i) -&gt; i + 1) .limit(10) .forEach(System.out::println)다른 예시도 알아보자.주사위를 100번 던져 6이 나오는 확률를 구해보자.Random r = new Random();var count : long = Stream.generate(() -&gt; r.nextInt(bound: 6) + 1) .limit(100) .filter(n -&gt; n === 6) .count();System.out.println(count); 스트림의 장점 : 연속된 데이터를 위에서 말한 고차함수들을 사용해 기능들을 간결하게 표현할 수 있다.익숙해지면 굉장히 편리하니 자주 사용하여 익숙해지자!Optional NPE : Null Pointer Exception-&gt; 가장 많이 발생하는 에러중 하나-&gt; 자바에서 거의 모든 것이 레퍼런스기 때문에 거의 모든것이 null이 될 수 있다.-&gt; 항상 null을 확인해야 한다! 그래서 null을 쓰지말자고 서로 약속한다. (계약한다.) 어떻게하면 null을 쓰지않을 수 있을까? 그 방법을 알아보자. EMPTY 객체 사용//EMPTY 정의public static final User EMPTY = new User(age:0, name:\"\");//초기값 설정User user = User.EMPTY;//사용예시if(user == User.EMPTY){ }이렇게 null을 사용하지 않고 pusblic static final로 EMPTY 초기값을 설정해준다. Opitonal 사용 Optinal이란? null를 포함한 객체를 이동시켜주는 바구니 로 하나의 type이다. 객체를 담아 이동시켜주는데 객체가 null일때 null을 보여주는 것이 아니라 아무것도 없는 바구니를 보여주는 것이다. 객체가 null이 아니면 바구니 속 data를 보여준다. 또한 간단한 기능들도 제공한다. Optional&lt;User&gt; optionalUser = Optinal.empty();OptionalUser = Optional.of(new User(age:1, name:\"2\"));optionalUser.isEmpty(); //null이면 trueoptionalUser.isPresent(); //값이 있으면 trueif (optionalUser.isPresent()) { //do1 } else{ //do2}// 위와 같은 if문을 Optional에서 제공하는 기능으로 사용할 수 있다.optionalUser.ifPresentOrElse(user -&gt; { //user라는 객체가 존재 //do 1}, () -&gt;{ //() : null값 //do 2})정리 Collection : 여러 데이터의 묶음 iterator : 데이터를 개별로 처리 stream : 데이터의 연속으로 데이터들을 다양한 고차함수들로 간결하게 표현 가능 Optional : 자바에서 많이 발생하는 에러인 NPE를 방지하기위해 개발자들간의 “null을 사용하지 말자” 는 암묵적인 rule이 생겼고 이를 위해 사용하는 type-&gt; 프로그램을 더 안전하게 만들 수 있게 되었다." }, { "title": "Day3.Interface", "url": "/posts/Java-for-framework-day3/", "categories": "Backend, Java for framework", "tags": "Java, Backend, SW", "date": "2022-07-06 17:29:00 +0900", "snippet": "[DAY3] Interface0. Interface란? 객체를 어떻게 구성해야하는지 알려주는 설계도interface interfaceName{ public static final int = 5; // int = 5; public abstract void Method1(); // void Method1();}이런 식으로 사용할 수 있다.주석과 같은 의미로 사용할 수 있는데 인터페이스에서 메소드는 무조건 추상메소드, int는 무조건 상수이기 때문에 public static final 과 public abstract void가 생략가능하다.1. Interface의 기능 Interface는 개발코드를 직접수정하지 않고 사용하고 있는 객체를 변경할 수 있게 한다.(1) 구현을 강제한다.#0에서 말했다시피 인터페이스는 추상메소드와 상수만 구현되어 객체를 어떻게 구성해야 하는지 알려준다. 따라서 이 인터페이스를 상속받은 클래스는 모든 메소드를 오버라이드하며 설계도를 따라 객체를 구성해나가야 하는 것이다. (2) 다형성을 제공한다.인테페이스의 가장 큰 장점이다. 인터페이스와 로직이 명확하게 분리되어 실제 메소드의 구현은 상속받은 자식 클래스에서 담당하기 때문에 외부에 노출할 필요 없는 로직을 캡슐화할 수 있다.(3) 결합도를 낮추는 효과 (의존성을 역전)이 부분이 가장 어려웠던 것 같다. Dependency Injection, Dependency Inversion등의 용어가 이해가 되지않았다.우선 Dependency (의존관계) 부터 천천히 알아보자.Dependency : 의존관계 의존 대상 B가 변경되었을 때 그 영향이 A에 미치는 관계예를 든 것중에 “음식레시피” 예시가 가장 이해가 잘되었다. 쿠키요리사는 쿠키레시피에 의존한다. 만약 쿠키레시피가 변경되다면 요리사는 쿠키를 새로운 방법으로 만들게 된다. “요리사는 레시피에 의존한다” 라고 말할 수 있는 것이다.public class CokkieChef { private HamburgerRecipe cokkieRecipe; public CokkieRecipe() { this.cookieRecipe = new CokkieRecipe(); //this.cookieRecipe = new ChockoCokkieRecipe(); }}이렇게 코드를 작성한다면 두 클래스 간 결합성이 높다는 문제가 발생한다.CookieChef클래스와 CokkieRecipe 클래스가 강하게 결합되어 초코쿠키 레시피를 사용하고 싶다면 주석처럼 CookieChef의 생성자를 직접 바꿔주어야 한다. 즉, 유연성이 떨어지는 것이다.이를 Dependency Injection가 해결할 수 있다.Dependency Injection (DI) : 의존관계 주입 의존관계를 외부해서 결정(주입) 해주는 것을 말한다.우선 다양한 쿠키 레시피를 Interface를 사용해 추상화 하자.public interface CookieRecipe{}public class ChockoCookieRecipe implements CookieRecipe{}그 후 CokkieChef 클래스의 생성자에서 외부로부터 쿠키레시피를 주입 (Injection) 받도록 하는 것이다.public class CookieChef{ private CookieRecipe cookieRecipe; public CookieRecipe(CookieRecipe cookieRecipe){ this.cookieRecipe = cookieRecipe; }}public class Customer{ private CookieChef cookieChef = new CookieChef(new CokkieRecipe()); public void orderMenu(){ cookieChef = new CokkieChef(new ChockoCokkieRecipe()); }}이렇게 되면 customer가 레시피를 결정해 Chef에게 주입할 수 있다. 이것이 Dependency Insection 이다.이때 Dependency Inversion이 일어나게 되는데Dependency Inversion : 의존관계 역전왼쪽 그럼처럼 A가 B(구상체)와 직접적으로 의존관계를 맺는 것이 아니라 오른쪽 그림처럼 interface라는 추상체를 중간에 둬서 추상체를 통하여 의존하게 하는 것이다. 이 때 발생하는것이 Dependency Inversion 이다. 이는 의존성을 역전해서 사용하라는 객체지향 5원칙중 DIP (Dependency Inversion Principle) 에 해당한다.2. default Method 기능 Interface에서 추상메소드가 아닌 메소드 구현을 할 수 있는 것이다.(java 8 부터 생성된 기능이다.)interface MyInterface { void method1(); // 추상메소드 default void sayHello { System.out.println(\"Hello World\"); }}public class Main implements MyInterface { public static void main(String[] args){ new Main().sayHello(); } @Override public void method1(){ throw new RuntimeException(); }}method1 처럼 추상메소드로 생성되어 Main에서 Override해주어야 하는 것이 아니라 defauld Method로 생성을 하면 기본적인 구현을 할 수 있고 Override하지 않아도 main에서 호출할 수 있다.그럼 어떨 때 default Method를 사용해야할까?interface MyInterface { void method1(); void method2();}public class Main { public static void main(String[] args) { new Service().method1(); }}class Service implements MyInterface { @ovveride public void method1() { System.out.println(\"Hello World\"); } @ovveride public void method2() { //(nothing) }}이 코드를 보게 되면 MyInterface에는 세가지의 추상메소드가 정의되어 있지만 정작 main에서는 method1만 사용하는걸 볼 수 있다.하지만 interface의 메소드를 모두 Override해야 하므로 method1을 사용하기 위해 불필요한 method2또한 Override를 해줘야 한다.(위 코드의 경우는 interface의 추상메소드가 두개 뿐이지만 메소드가 많을수록 불필요하게 Override해야할 메소드가 많아지게 되는 것이다.)이를 해결하기 위한 것이 Adaptor이다.Adaptorpublic class MyInterfaceAdaptor implements MyInterface{ @Override public void method1() { } @Override public void method2() { }}class Service extends MyInterfaceAdapter { @ovveride public void method1() { System.out.println(\"Hello World\"); }}MyInterface를 Adaptor를 통해 inplements 하여 모든 메소드를 비어있는 상태로 구현한다.그 후 Service에서는 implements가 아닌 extends, 상속을 하여 method1만 Override하여 사용하는 것이다.그러면 이전처럼 불필요한 method2를 구현하지 않아도 된다.하지만 이 또한 문제가 생길 수 있다.만약 Service가 다른 object를 상속받았다고 하자. Java에서 extends는 하나만 가능한데 그럼 다시 method1을 쓰기위해 MyInterfaceAdator를 implements받아야 한다. 그렇게 되면 또 똑같이 불필요한 method2도 Override해야 하는 문제가 발생하는 것이다. 이를 위한 것이 바로 default Method이다.default Methodinterface MyInterface { default void method1(){}; default void method2(){};}class Service extends Object implements MyInterface { @ovveride public void method1() { System.out.println(\"Hello World\"); }}interface의 메소드를 default Method로 작성해 주면 이를 implements 받은 class에서 원하는 메소드만 Override해 사용할 수 있다. default Method 기능을 정리하자면 Adaptor의 역할을 할 수 있다. Override할 때 구현되는 기능이 항상 같다면, default Method로 한번만 구현하면 Override없이 그저 implements하는 것만으로도 그 기능을 사용할 수있다. 2-1. Static Methoddefault Method처럼 Static Method도 있다.public interface Ability { static void sayHello() { System.out.println(\"Hello World\"); }}public class Main { public static void main(String[] args) { Ability.sayHello(); }}특정 기능을 수행할 수 있는 Static Method를 interface에 담고 이를 호출하는 것으로 사용할 수 있다. 함수의 역할과 같으므로 함수제공자라고 생각하면 된다.3. Functional Interface4. Lambda 표현식" }, { "title": "Git repository 정리하기", "url": "/posts/Git-repository-clean-up/", "categories": "Git", "tags": "GitHub", "date": "2022-07-06 13:06:00 +0900", "snippet": "repository 병합하기 git을 본격적으로 사용하다보니 처음 git을 시작했을때 생각없이 올린 repository를 정리해 카테고리별로 합쳐야 겠다는 생각이 들었다. repository들을 모두 clone하여 하나의 폴더에 합치고 다시 push하기엔 일이 너무 많을 것 같아 git bash로 병합하는 법을 알아보자!OOP_2020 repository에 OOP_Training_03 repository를 폴더로 넣어줄 것 이다.Step 1. 남겨둘 repository로 이동한다.$cd [남겨둘 repository경로]$cd ~/Desktop/pre_project/OOP_2020OOP_2020 repository에 합칠 것이기 때문에 위의 경로로 설정하였다.Step 2. repository를 병합한다.$git remote add [합칠 repository 이름] [합칠 repository 주소]$git fetch [합칠 repository 이름]$git merge --allow-unrelated-histories [합칠 repository 이름/합칠 branch 이름]git bash에 이와같이 작성하면 된다.(git bash에 복사한 주소를 붙여넣을 때는 ctrl + shift + insert 사용한다.)$git remote add OOP_Training_03 https://github.com/ssstopeun/OOP_Training_03.git$git fetch OOP_Training_03$git merge --allow-unrelated-histories OOP_Training_03/master내 경우 이렇게 해주었다.합쳐진 repository는 git에서 setting으로 직접 삭제했다. git bash로 삭제하는 법도 있겠지만… 그냥 지울래.마무리처음에 아무것도 모르고 push 했던 것들 때문에 정리하는 시간을 가져보았다. 이제 한 카테고리 정리해서 나머지도 얼른 정리해야겠다.하고보니 별게 아닌데 처음엔 /master인데 /main으로 하는 등 오류가 장난아니었다. 다음에 병합할땐 보다쉽게 할 수 있겠지?ㅠㅠ. git에 익숙해지자!" }, { "title": "Day1.Special", "url": "/posts/Java-for-framework-day1-2/", "categories": "Backend, Java for framework", "tags": "Java, Backend, SW, SW Developer", "date": "2022-07-04 16:30:00 +0900", "snippet": "[Day1.Special] 초보개발자가 알아야 할 것1. Coding Convention 팀이나 회사와 같은 개발 그룹에서 정해서 사용한다.정하지 않을 경우 =&gt; 일반적인 자바 코딩 룰을 따른다.(1) 클래스명은 대문자로 시작한다.class myClass {} // (x)class My_Class {} // (x)class MyClass {} // (o)(2) 메소드나 변수명은 소문자로 시작한다.int my_variable = 0; //(x)int myVariable = 0; //(o)void GoHome() {} //(x)void goHome() {} //(o)(3) Indent : 들여쓰기 Tab or Space (섞어서 쓰기 금지!) VSCode에서는 Tab = Space * 4 이지만 이는 시스템에 따라 다르기 때문에 한가지만 쓰는 것이 중요하다.2. Reference(Q1) Java에서는 alloc / free 를 개발자가 일일히 신경쓰지 않아도 된다. O(Q2) Java를 하면 포인터를 몰라도 될까? X Java에는 포인터대신 레퍼런스 라는 개념이 있기 때문에 이 개념을 정확히 알아야 한다.Reference란? java에서는 모든것이 레퍼런스 값이다. 정확하게 말하면 primitive한 8개를 제외한 것이 레퍼런스이다. primitive : boolean, byte, int short, long, float, double, char array 는 reference처럼 취급한다. (Int [] 도 reference취급) Call by value / Call by reference코드를 보며 reference에 대해 이해해보자.public class ByeWorld{ public static void main(String[] args){ ByeWorld b = new ByeWorld(); int a = 100; b.doSomething(a); System.out.println(a); //main a에는 변화가 없다. a = 100 } private void doSomething(int a) { //doSomething의 a와 main의 a는 다른 곳에 저장되어있다. a *= 2; //doSomething의 a = 200 }} Call by value다음 코드에서는 doSomething을 호출해 a의 값을 2배로 늘렸다고 생각하겠지만 main의 a와 doSomething의 a의 메모리가 다르기 때문에 결국에는 main의 a에는 변화가 생기지 않는다.그렇다면 a가 같은 메모리를 참조해 a의 값이 2배로 늘게 하려면 어떻게 해야할까?class Int { int a = 100; //Int 라는 object생성}public class ByeWorld{ public static void main(String[] args){ ByeWorld b = new ByeWorld(); int a = new Int(); //a는 Int를 가르킨다. b.doSomething(a); System.out.println(a.a); } private void doSomething(Int a) { //Int를 가르킨다. a *= 2; }} Call by reference이렇게 Int라는 object를 생성하여 사용하게 되면 main의 a와 doSomething의 a의 메모리는 다르겠지만 두 메모리가 가르키는 곳은 Int로 같다. 따라서 같은 data를 참조하고 변화시키기 때문에 a의 값이 200으로 나오게 되는 것이다.3. Contant Pool Java에서는 Constant Pool을 이용해 String을 특별취급한다.따라서 String 객체는 한번 값이 할당되면 그 공간이 변하지 않는다. (불변성)예시코드를 보자.String a = \"\";for (int i = 0; i &lt; 10; i++){ a += 1;}System.out.println(a);결과는 0123456789다.a = ““가 저장된 공간에서 “0”, “01”, “012” … 이런식으로 값만 계속 바뀔 것이라고 생각한다.하지만 Java에서는 이를 Constant Pool공간에 “”, “0”, “01” … 총 11개의 값이 모두 따로 저장된다.=&gt; 메모리사용에 있어 비효율적이다.String a = \"Hello World\";String b = \"Hello World\";System.out.println(a == b); //reference가 같은지 비교그럼 이경우는 어떨까? a = “Hello World” 따로 b = “Hello World” 따로 저장되었을 것이라고 생각한다.하지만 java에서는 우선 Constant Pool에 “Hello World”라는 글자를 만들고 a가 이를 가르킨다. b는 Constant Pool에 똑같은 것을 찾게 되면 그 메모리를 참조하여 사용한다.=&gt; 메모리를 효율적으로 사용할 수 있다.첫번째 코드의 비효율성을 어떻게 해결할 수 있을까? 조합을 다한 후 한번만 Constant Pool에 저장하자.=&gt; StringBuffer를 사용StringBuffer는 한번값이 할당되더라도 다른 값이 할당되면 공간이 변한다. (가변성) StringBuffer StringBuffer sb = new StringBuffer();for (int i = 0; i &lt; 10; i++){ sb.append(i);}System.out.println(sb); 결과는 위와 동일하다. 하지만 Constant Pool에는 “0123456789” 라는 값만 저장되어 메모리를 효율적으로 사용할 수 있다. 비슷한 기능으로 StringBuilder도 있는데 StringBuffer와의 차이를 살펴보자.##StringBuffer VS StringBuilder 값이 변경되었을때 같은 주소공간을 참조 하고 값이 변경되는 가변성을 띈다는 점에서는 같다. 차이점은 바로 동기화 (Synchronization)이다. StringBuilder는 동기화를 지원하지 않고 StringBuffer는 동기화를 지원한다. 동기화를 지원한다는 것은 멀티 스레드 환경에서도 안전하게 동작할 수 있다는 말이다. StringBuffer는 동기화를 위해 synchronized 키워드를 사용한다. synchronized : 여러개의 스레드가 한 개의 자원에 접근하려고 할 때, 현재 사용하고 있는 스레드를 제외한 나머지 스레드들이 데이터에 접근할 수 없도록 막는 역할을 한다. 멀티 스레드 환경에서 A, B 스레드가 StringBuffer의 append()메소드를 사용하고자 한다고 해보자. A 스레드 : sb의 append() 동기화 블록에 접근 및 실행 B 스레드 : A 스레드 sb 의 append() 동기화 블록에 들어가지 못하고 block 상태가 됨. A 스레드 : sb의 append() 동기화 블록에서 탈출 B 스레드 : block 에서 running 상태가 되며 sb 의 append() 동기화 블록에 접근 및 실행. 하지만 속도측면에서는 StringBuilder가 더 빠르다. 정리하자면 변하지 않는 문자열을 자주 사용할 경우=&gt; String 단일 스레드 환경이고 문자열이 계속 변할 경우=&gt; StringBuilder 멀티 스레드 환경이고 문자열이 계속 변할 경우=&gt; StringBuffer 4. Object 모든 객체의 최상위 객체 이다.모든 객체에는 Object의 메소드를 호출할 수 있다.=&gt; Object의 메소드 종류, 기능을 명확히 알아야 한다. 대표적인 메소드 toString() equals() hashCode() 5. Git git은 기본! 명령어로 익힐 필요는 없지만 어떻게 사용하는지는 알아야 한다. ex. branch는 어떻게 따고, branch가 merge되면 어떻게 되고, 충돌이 나면 어떻게 해야하고 등등 git tool 사용 GitHub Desktop Sourcetree intellij Extension -&gt; Git graph .gitignore을 잘 활용하자! gitignore : 포함되지 않아도 되는 파일들을 관리할때 사용 build결과 (*.class, *.jar, build/), generate가능한 파일, local설정, 키/보안관련 .gitignore에 어떤 파일을 넣어햐 하나요? https://gitignore.io 를 통해 본인의 작업파일에서의 필요한 .gitignore파일 내용 생성이 가능하다. 정리 자바 개발자라면 기본적으로 지켜야할 것이 있다. 자바 개발을 위해서는 Reference의 개념을 명확히 알고 사용할 줄 알아야 한다. Call by value와 Call by reference의 차이를 알아야 한다. String, StringBuffer, StringBuilder의 차이를 알고 상황에 맞춰 사용하자. Object는 모든 객체의 최상위 객체로서 알고있으면 활용범위가 넓어지므로 Object의 메소드와 기능들에 대해 학습해야한다. git은 개발자의 기본이므로 사용법을 잘 익혀 활용할 줄 아는 것이 중요하다. git를 센스있게 사용하기 위해서는 .gitignore을 잘 사용해야 한다." }, { "title": "Day1.Java", "url": "/posts/Java-for-framework-day1/", "categories": "Backend, Java for framework", "tags": "Java, Backend, SW, Gradle", "date": "2022-07-04 14:00:00 +0900", "snippet": "[Day1] JAVAJAVA 개발환경 JDK (Java Development Kit)JVM (Java Virtual Machine) 필요 =&gt; 실행환경 : JRE (Java Runtime Environment)JRE + 개발 tool =&gt; 개발환경 : JDK Oracle JDK : https://www.oracle.com/java/technologies/downloads/특정 라이센스를 이용하기 위해서는 Oracle라이센스를 구매해야 한다. Open JDK : https://openjdk.org/무료 오픈 소스이다. 다운을 받은 후 path설정 등의 과정을 거쳐야 사용할 수 있고 cmd창에 java –version을 통해 잘 설정이 되었는지 확인할 수 있다. VSCode EXTENSION에서 java extension pack 설치하기 Build Tool 자동으로 Build, 실행해주는 toolex. Ant, Maven, Gradle Gradle : https://gradle.org Gradle 사용 cmd창에서 build할 폴더로 이동 gradle init 으로 프로젝트 생성 gradle tasks 로 데스크 목록 확인 gradle run 으로 실행 IDE 통합개발환경 ex. Eclipse, Intellij Intellij : http://www.jetbrains.com/ko-kr/idea psvm / sout 등의 키워드로 빠른 코딩이 가능하다. -&gt; 생산성 향상 유용한 단축키 jetbrain에서 제공하는 단축키 정리 pdf https://resources.jetbrains.com/storage/products/intellij-idea/docs/IntelliJIDEA_ReferenceCard.pdf 대표적 단축키 단축키 설명 Alt+Enter 빠른 수정 Ctrl+1 폴더창으로 커서 이동 (&lt;-&gt; Esc) Ctrl+N 새파일 생성 Shift+Shift 폴더창에서 파일탐색 Alt+Up/Down* 단계별 블록지정 Ctrl+/ 주석지정 ctrl+Alt+L 코드 reformating Shift+Ctrl+Alt+T refactoring (Intellij의 특징적인 기능) Shift+Ctrl+A : 명령어 검색 명령어검색기능 (다른 기능기억못해도 이것만 기억하면 검색하여 사용가능!) 정리 JAVA 개발환경을 위해 jdk를 다운받아 나의 개발환경을 꾸미자. 자동으로 빌드와 실행을 해주는 Build tool 중 하나인 Gradle을 사용해보자. 자바개발자들이 주로 사용하는 IDE인 intellij와 이 환경에서 사용하는 단축키를 알아보자. 단축키는 Shift+Ctrl+A로 검색이 가능하고 Intellij Cheat Sheet에 모든 단축키가 나와있다." } ]
